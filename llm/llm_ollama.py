# ===================================================================
# LLM_OLLAMA.PY - OBSÅUGA LOKALNEGO MODELU OLLAMA
# ===================================================================
# Wersja: AIA v2.1
# Opis: Interfejs do lokalnych modeli LLM przez Ollama API
# Endpoint: http://localhost:11434
# ===================================================================

import requests
import json
import time
from typing import Dict, Any, Optional

class OllamaError(Exception):
    """BÅ‚Ä™dy zwiÄ…zane z Ollama"""
    pass

def sprawdz_polaczenie(base_url: str = "http://localhost:11434") -> bool:
    """
    Sprawdza czy serwer Ollama dziaÅ‚a
    
    Args:
        base_url (str): URL serwera Ollama
        
    Returns:
        bool: True jeÅ›li serwer odpowiada
    """
    try:
        response = requests.get(f"{base_url}/api/tags", timeout=5)
        return response.status_code == 200
    except requests.RequestException:
        return False

def lista_modeli(base_url: str = "http://localhost:11434") -> list:
    """
    Pobiera listÄ™ dostÄ™pnych modeli
    
    Args:
        base_url (str): URL serwera Ollama
        
    Returns:
        list: Lista nazw modeli
    """
    try:
        response = requests.get(f"{base_url}/api/tags", timeout=10)
        response.raise_for_status()
        
        data = response.json()
        return [model["name"] for model in data.get("models", [])]
    except requests.RequestException as e:
        raise OllamaError(f"Nie moÅ¼na pobraÄ‡ listy modeli: {e}")

def ollama_generate(prompt: str, config: Dict[str, Any]) -> str:
    """
    Generuje odpowiedÅº uÅ¼ywajÄ…c modelu Ollama
    
    Args:
        prompt (str): Tekst zapytania
        config (dict): Konfiguracja z ustawieniami modelu
        
    Returns:
        str: OdpowiedÅº modelu
        
    Raises:
        OllamaError: JeÅ›li wystÄ…pi bÅ‚Ä…d API
    """
    
    # WyciÄ…gnij ustawienia z config
    llm_config = config.get("llm_config", {})
    base_url = llm_config.get("base_url", "http://localhost:11434")
    model = llm_config.get("model", "llama3.1:8b")
    max_tokens = llm_config.get("max_tokens", 2048)
    temperature = llm_config.get("temperature", 0.7)
    
    # SprawdÅº poÅ‚Ä…czenie
    if not sprawdz_polaczenie(base_url):
        raise OllamaError("Serwer Ollama nie odpowiada. SprawdÅº czy dziaÅ‚a: ollama serve")
    
    # SprawdÅº czy model istnieje
    dostepne_modele = lista_modeli(base_url)
    if model not in dostepne_modele:
        raise OllamaError(f"Model '{model}' nie jest dostÄ™pny. DostÄ™pne: {dostepne_modele}")
    
    # Przygotuj Å¼Ä…danie
    url = f"{base_url}/api/generate"
    data = {
        "model": model,
        "prompt": prompt,
        "stream": False,
        "options": {
            "num_predict": max_tokens,
            "temperature": temperature
        }
    }
    
    start_time = time.time()
    
    try:
        print(f"ğŸ§  Pytam Ollama {model} (local)...")
        
        response = requests.post(url, json=data, timeout=60)
        response.raise_for_status()
        
        result = response.json()
        
        # SprawdÅº czy odpowiedÅº zawiera bÅ‚Ä…d
        if "error" in result:
            raise OllamaError(f"BÅ‚Ä…d modelu: {result['error']}")
        
        odpowiedz = result.get("response", "").strip()
        
        # Statystyki
        elapsed_time = time.time() - start_time
        total_duration = result.get("total_duration", 0) / 1e9  # nanosekund â†’ sekundy
        
        print(f"âœ… Ollama odpowiada ({elapsed_time:.1f}s): {odpowiedz[:50]}...")
        
        return odpowiedz
        
    except requests.exceptions.Timeout:
        raise OllamaError("Timeout - model zbyt dÅ‚ugo generuje odpowiedÅº")
    except requests.exceptions.ConnectionError:
        raise OllamaError("Brak poÅ‚Ä…czenia z serwerem Ollama")
    except requests.exceptions.HTTPError as e:
        raise OllamaError(f"BÅ‚Ä…d HTTP: {e}")
    except json.JSONDecodeError:
        raise OllamaError("NieprawidÅ‚owa odpowiedÅº JSON z serwera")

def odpowiedz(prompt: str, config: Dict[str, Any]) -> str:
    """
    GÅ‚Ã³wna funkcja interfejsu - kompatybilna z llm_openrouter.py
    
    Args:
        prompt (str): Tekst zapytania
        config (dict): Konfiguracja systemu
        
    Returns:
        str: OdpowiedÅº modelu lub komunikat bÅ‚Ä™du
    """
    try:
        return ollama_generate(prompt, config)
        
    except OllamaError as e:
        error_msg = f"[BÅ‚Ä…d Ollama]: {str(e)}"
        print(f"âŒ {error_msg}")
        return error_msg
        
    except Exception as e:
        error_msg = f"[BÅ‚Ä…d nieoczekiwany]: {str(e)}"
        print(f"âŒ {error_msg}")
        return error_msg

def test_ollama():
    """
    Funkcja testowa - sprawdza czy Ollama dziaÅ‚a
    """
    print("ğŸ§ª Test moduÅ‚u Ollama...")
    
    # Test poÅ‚Ä…czenia
    if sprawdz_polaczenie():
        print("âœ… Serwer Ollama dziaÅ‚a")
    else:
        print("âŒ Serwer Ollama nie odpowiada")
        return False
    
    # Test modeli
    try:
        modele = lista_modeli()
        print(f"âœ… DostÄ™pne modele: {modele}")
        
        if not modele:
            print("âŒ Brak modeli - pobierz: ollama pull llama3.1:8b")
            return False
            
    except OllamaError as e:
        print(f"âŒ BÅ‚Ä…d: {e}")
        return False
    
    # Test generacji
    test_config = {
        "llm_config": {
            "model": modele[0],  # Pierwszy dostÄ™pny model
            "max_tokens": 50,
            "temperature": 0.7
        }
    }
    
    try:
        odpowiedz_test = odpowiedz("CzeÅ›Ä‡! Odpowiedz krÃ³tko po polsku.", test_config)
        print(f"âœ… Test odpowiedzi: {odpowiedz_test}")
        return True
        
    except Exception as e:
        print(f"âŒ BÅ‚Ä…d testu: {e}")
        return False

# ===================================================================
# TESTY LOKALNE
# ===================================================================

if __name__ == "__main__":
    print("ğŸ§ª Testowanie moduÅ‚u llm_ollama.py")
    
    if test_ollama():
        print("ğŸ‰ Ollama gotowy do uÅ¼ycia!")
    else:
        print("âŒ Ollama wymaga konfiguracji")
        print("SprawdÅº:")
        print("1. ollama serve (czy dziaÅ‚a)")
        print("2. ollama list (czy masz modele)")
        print("3. ollama pull llama3.1:8b (pobierz model)")

# ===================================================================
# KONIEC PLIKU
# ===================================================================