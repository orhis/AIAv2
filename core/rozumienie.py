# ===================================================================
# CORE/ROZUMIENIE.PY - G≈Å√ìWNY MODU≈Å PRZETWARZANIA JƒòZYKA NATURALNEGO
# ===================================================================
# Wersja: AIA v2.1 + Ollama Support
# Opis: System 3-poziomowego rozpoznawania intencji + zarzƒÖdzanie tokenami + Ollama
# Komponenty: TokenManager, LLM Intent Classifier, RAG, Intencje lokalne, Ollama
# ===================================================================

import json
import re
import time
import os
from llm import llm_openrouter

# Import Ollama z fallback
try:
    from llm import llm_ollama
    OLLAMA_AVAILABLE = True
    print("‚úÖ Modu≈Ç Ollama za≈Çadowany")
except ImportError:
    OLLAMA_AVAILABLE = False
    print("‚ö†Ô∏è Modu≈Ç Ollama niedostƒôpny")

from core import logger

# ===================================================================
# IMPORT UNIVERSAL INTELLIGENT ASSISTANT - NOWY SYSTEM
# ===================================================================

# Import Universal Assistant z fallback
try:
    from core.universal_intelligent_assistant import universal_intelligent_assistant
    UNIVERSAL_ASSISTANT_AVAILABLE = True
    print("‚úÖ Universal Intelligent Assistant za≈Çadowany")
except ImportError:
    UNIVERSAL_ASSISTANT_AVAILABLE = False
    print("‚ö†Ô∏è Universal Assistant niedostƒôpny - u≈ºywam klasycznego systemu")

# Konfiguracja Universal Assistant
UNIVERSAL_ASSISTANT_CONFIG = {
    "enabled": True,  # Domy≈õlnie w≈ÇƒÖczony
    "fallback_to_classic": True,  # Fallback na klasyczny system przy b≈Çƒôdzie
    "auto_context_detection": True,  # Automatyczne wykrywanie kontekstu
    "supported_contexts": ["cooking", "smart_home", "calendar", "finance", "general"]
}

print(f"ü§ñ Universal Assistant: {'ENABLED' if UNIVERSAL_ASSISTANT_AVAILABLE else 'DISABLED'}")

# ===================================================================
# üìã SEKCJA 1: FUNKCJE POMOCNICZE - PRZETWARZANIE TEKSTU
# ===================================================================

def wypowiedz_bez_prefixow(odpowiedz):
    """
    Usuwa prefixy wizualne przed wys≈Çaniem do TTS
    
    Usuwa znaczniki:
    - ü§ñ [LLM za intencjƒô 'nazwa']: 
    - üß† [Czysty LLM]: 
    - üß† [Ollama LLM]:  # ‚Üê DODANE
    
    Returns:
        str: Tekst bez prefix√≥w wizualnych
    """
    clean_text = re.sub(r'^ü§ñ \[LLM za intencjƒô.*?\]: ', '', odpowiedz)
    clean_text = re.sub(r'^üß† \[Czysty LLM\]: ', '', clean_text)
    clean_text = re.sub(r'^üß† \[Ollama LLM\]: ', '', clean_text)  # ‚Üê DODAJ Tƒò LINIƒò
    return clean_text

# ===================================================================
# üîê SEKCJA 2: TOKEN MANAGER - ZARZƒÑDZANIE LIMITAMI API
# ===================================================================

class TokenManager:
    """
    Klasa zarzƒÖdzajƒÖca limitami token√≥w dla r√≥≈ºnych modeli LLM
    
    Funkcje:
    - Auto-wykrywanie limit√≥w z b≈Çƒôd√≥w 402
    - Persistent cache w data/token_limits.json
    - Bezpieczne startowe warto≈õci
    - Fallback miƒôdzy modelami
    """
    
    def __init__(self):
        self.cache_file = "data/token_limits.json"
        self.model_limits = self.load_cache()
        self.default_safe_tokens = 150  # Bezpieczny start
        
    def load_cache(self):
        """Wczytuje zapisane limity token√≥w z pliku"""
        try:
            if os.path.exists(self.cache_file):
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            print(f"‚ö†Ô∏è Nie mo≈ºna wczytaƒá cache token√≥w: {e}")
        return {}
    
    def save_cache(self):
        """Zapisuje limity token√≥w do pliku"""
        try:
            os.makedirs(os.path.dirname(self.cache_file), exist_ok=True)
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.model_limits, f, indent=2)
        except Exception as e:
            print(f"‚ö†Ô∏è Nie mo≈ºna zapisaƒá cache token√≥w: {e}")
    
    def get_safe_tokens(self, model, requested_tokens=2048):
        """
        Zwraca bezpiecznƒÖ liczbƒô token√≥w dla modelu
        
        Args:
            model (str): Nazwa modelu (np. "openai/gpt-4-turbo")
            requested_tokens (int): ≈ªƒÖdana liczba token√≥w
            
        Returns:
            int: Bezpieczna liczba token√≥w
        """
        if model in self.model_limits:
            safe_limit = min(requested_tokens, self.model_limits[model])
            print(f"üîí Model {model}: u≈ºywam zapisanego limitu {safe_limit} token√≥w")
            return safe_limit
        else:
            print(f"üÜï Model {model}: pierwsza pr√≥ba z {self.default_safe_tokens} token√≥w")
            return min(requested_tokens, self.default_safe_tokens)
    
    def handle_402_error(self, model, error_message):
        """
        WyciƒÖga i zapisuje rzeczywisty limit z b≈Çƒôdu 402
        
        Args:
            model (str): Nazwa modelu
            error_message (str): Komunikat b≈Çƒôdu z API
            
        Returns:
            int: Rzeczywisty limit token√≥w
        """
        match = re.search(r"can only afford (\d+)", error_message)
        if match:
            actual_limit = int(match.group(1))
            self.model_limits[model] = actual_limit
            self.save_cache()
            print(f"üíæ Zapisano limit dla {model}: {actual_limit} token√≥w")
            return actual_limit
        return self.default_safe_tokens

# Globalna instancja Token Manager
token_manager = TokenManager()

# ===================================================================
# üéõÔ∏è SEKCJA 2.5: INTENT RECOGNIZER - SWITCHER METOD ROZPOZNAWANIA
# ===================================================================

class IntentRecognizer:
    """
    Klasa zarzƒÖdzajƒÖca r√≥≈ºnymi metodami rozpoznawania intencji
    
    Obs≈Çuguje:
    - regex_only: tylko wzorce regex
    - regex_plus_simple: regex + prosty LLM
    - regex_plus_few_shot: regex + few-shot LLM
    """
    
    def __init__(self, config):
        self.method = config.get("recognition_config", {}).get("method", "regex_plus_simple")
        self.confidence_threshold = config.get("recognition_config", {}).get("confidence_threshold", 0.7)
        self.use_context = config.get("recognition_config", {}).get("use_context", False)
        self.debug_mode = config.get("recognition_config", {}).get("debug_mode", False)
        self.previous_context = ""
        
        if self.debug_mode:
            print(f"üéõÔ∏è IntentRecognizer: method={self.method}, confidence={self.confidence_threshold}")
    
    def classify_intent(self, tekst, dostepne_intencje, config):
        """G≈Ç√≥wna funkcja klasyfikacji z wyborem metody"""
        
        if self.debug_mode:
            print(f"üîç Debug: Klasyfikujƒô '{tekst}' metodƒÖ {self.method}")
        
        # KROK 1: Zawsze sprawd≈∫ regex (najszybsze)
        regex_result = self._try_regex(tekst, dostepne_intencje)
        if regex_result:
            if self.debug_mode:
                print(f"‚úÖ Debug: Regex znalaz≈Ç '{regex_result}'")
            return regex_result, "regex"
        
        # KROK 2: Je≈õli method == "regex_only", ko≈Ñczymy
        if self.method == "regex_only":
            if self.debug_mode:
                print("‚ùå Debug: Regex_only - brak dopasowania")
            return None, "regex_only"
        
        # KROK 3: Pr√≥buj LLM classifier
        if self.method in ["regex_plus_simple", "regex_plus_few_shot"]:
            llm_result = self._try_llm_classifier(tekst, dostepne_intencje, config)
            if llm_result:
                if self.debug_mode:
                    print(f"‚úÖ Debug: LLM znalaz≈Ç '{llm_result}'")
                return llm_result, "llm_classifier"
        
        if self.debug_mode:
            print("‚ùå Debug: ≈ªadna metoda nie znalaz≈Ça intencji")
        return None, "no_match"
    
    def _try_regex(self, tekst, dostepne_intencje):
        """Pr√≥buje dopasowaƒá regex patterns"""
        for komenda in dostepne_intencje:
            try:
                wzorzec = komenda["wzorzec"]
                intencja = komenda["intencja"]
                if re.search(wzorzec, tekst, re.IGNORECASE):
                    return intencja
            except (KeyError, re.error):
                continue
        return None
    
    def _try_llm_classifier(self, tekst, dostepne_intencje, config):
        """Pr√≥buje LLM classifier"""
        if self.method == "regex_plus_simple":
            return klasyfikuj_intencje_llm_simple(tekst, dostepne_intencje, config)
        elif self.method == "regex_plus_few_shot":
            return klasyfikuj_intencje_llm_few_shot(tekst, dostepne_intencje, config)
        return None

# Globalna instancja (zostanie utworzona w analizuj())
intent_recognizer = None

# ===================================================================
# üß† SEKCJA 3: RAG SYSTEM - INICJALIZACJA I KONFIGURACJA  
# ===================================================================

try:
    import sys
    import os
    rag_path = os.path.join(os.path.dirname(__file__), 'rag')
    sys.path.insert(0, rag_path)
    from rag_engine import RecipeRAG
    RAG_AVAILABLE = True
    
    # Inicjalizacja RAG - raz przy starcie
    recipe_rag = RecipeRAG()
    if recipe_rag.initialize():
        print("‚úÖ RAG zainicjalizowany w rozumieniu")
    else:
        print("‚ùå B≈ÇƒÖd inicjalizacji RAG")
        RAG_AVAILABLE = False
except Exception as e:
    print(f"‚ö†Ô∏è RAG niedostƒôpny: {e}")
    RAG_AVAILABLE = False
    recipe_rag = None

# ===================================================================
# RAG ADAPTER DLA UNIVERSAL ASSISTANT - KOMPATYBILNO≈öƒÜ
# ===================================================================
class RagEngineAdapter:
    """
    Adapter dla kompatybilno≈õci miƒôdzy RecipeRAG a Universal Assistant
    Konwertuje interfejs RecipeRAG na RagEngine wymagany przez Universal Assistant
    """
    def __init__(self, recipe_rag_instance):
        self.recipe_rag = recipe_rag_instance
    
    def search_relevant(self, query):
        """Interfejs wymagany przez Universal Assistant"""
        if not self.recipe_rag:
            return []
        
        try:
            # U≈ºyj istniejƒÖcej funkcji suggest_recipes
            result = self.recipe_rag.suggest_recipes([query], max_results=5)
            recipes = result.get('all_recipes', [])
            
            print(f"üîç RAG Adapter: '{query}' ‚Üí {len(recipes)} wynik√≥w")
            return recipes
            
        except Exception as e:
            print(f"‚ùå B≈ÇƒÖd RAG Adapter: {e}")
            return []

# Stw√≥rz adapter dla Universal Assistant
if RAG_AVAILABLE and recipe_rag:
    rag_adapter = RagEngineAdapter(recipe_rag)
    print("‚úÖ RAG Adapter utworzony dla Universal Assistant")
else:
    rag_adapter = None
    print("‚ö†Ô∏è RAG Adapter niedostƒôpny")

# ===================================================================
# üìö SEKCJA 4: KOMENDY PREDEFINIOWANE - WCZYTANIE I WALIDACJA
# ===================================================================

try:
    with open("config/komendy_domyslne.json", encoding="utf-8") as f:
        KOMENDY = json.load(f)
    print(f"‚úÖ Wczytano {len(KOMENDY)} predefiniowanych komend")
except FileNotFoundError:
    print("‚ö†Ô∏è Brak pliku komendy_domyslne.json - u≈ºywam trybu tylko LLM")
    KOMENDY = []
except json.JSONDecodeError as e:
    print(f"‚ùå B≈ÇƒÖd w pliku komendy_domyslne.json: {e}")
    KOMENDY = []

# ===================================================================
# ü§ñ SEKCJA 5: LLM PROVIDERS - SWITCHER MIƒòDZY OLLAMA I OPENROUTER
# ===================================================================

def zapytaj_llm_safe(tekst, config, max_retries=2):
    """
    Uniwersalna funkcja LLM z auto-switcherem provider
    
    Obs≈Çuguje:
    - provider: "openrouter" ‚Üí llm_openrouter
    - provider: "ollama" ‚Üí llm_ollama (lokalny)
    - Auto-fallback miƒôdzy providerami
    
    Args:
        tekst (str): Tekst zapytania
        config (dict): Konfiguracja z provider i model
        max_retries (int): Maksymalne pr√≥by
        
    Returns:
        str: Odpowied≈∫ LLM lub komunikat b≈Çƒôdu
    """
    
    provider = config.get("llm_config", {}).get("provider", "openrouter")
    model = config.get("llm_config", {}).get("model", "")
    
    print(f"üéØ Provider: {provider}, Model: {model}")
    
    # ===============================================================
    # OLLAMA (LOKALNY)
    # ===============================================================
    if provider == "ollama":
        if not OLLAMA_AVAILABLE:
            print("‚ùå Ollama nie jest dostƒôpny - fallback na OpenRouter")
            return zapytaj_openrouter_safe(tekst, config, max_retries)
        
        try:
            return llm_ollama.odpowiedz(tekst, config)
        except Exception as e:
            print(f"‚ùå B≈ÇƒÖd Ollama: {e}")
            
            # Fallback na OpenRouter je≈õli Ollama nie dzia≈Ça
            print("üîÑ Fallback na OpenRouter...")
            return zapytaj_openrouter_safe(tekst, config, max_retries)
    
    # ===============================================================
    # OPENROUTER (CHMURA)
    # ===============================================================
    else:
        return zapytaj_openrouter_safe(tekst, config, max_retries)

def zapytaj_openrouter_safe(tekst, config, max_retries=2):
    """
    Bezpieczne zapytanie OpenRouter LLM z auto-adjustem token√≥w
    (stara funkcja zapytaj_llm_safe, ale tylko dla OpenRouter)
    """
    
    model = config["llm_config"]["model"]
    requested_tokens = config["llm_config"].get("max_tokens", 2048)
    
    for attempt in range(max_retries):
        # Pobierz bezpiecznƒÖ liczbƒô token√≥w
        safe_tokens = token_manager.get_safe_tokens(model, requested_tokens)
        
        # Aktualizuj config z bezpiecznymi tokenami
        temp_config = config.copy()
        temp_config["llm_config"]["max_tokens"] = safe_tokens
        
        try:
            print(f"üß† Pytam {model} (tokens: {safe_tokens})...")
            odpowiedz = llm_openrouter.odpowiedz(tekst, temp_config)
            
            # Sprawd≈∫ czy odpowied≈∫ zawiera b≈ÇƒÖd 402
            if "[B≈ÇƒÖd API OpenRouter: 402]" in odpowiedz and "can only afford" in odpowiedz:
                # WyciƒÖgnij liczbƒô token√≥w z b≈Çƒôdu
                actual_limit = token_manager.handle_402_error(model, odpowiedz)
                
                print(f"üîÑ Wykryto limit {actual_limit} token√≥w, ponawiam zapytanie...")
                
                # Je≈õli to pierwsza pr√≥ba, spr√≥buj ponownie
                if attempt == 0:
                    continue
                else:
                    return f"‚ùå Brak kredyt√≥w API. Dostƒôpne tokeny: {actual_limit}. Do≈Çaduj konto na https://openrouter.ai/settings/credits"
            
            elif odpowiedz.startswith("[B≈ÇƒÖd"):
                # Inny b≈ÇƒÖd API
                if attempt == 0:
                    print(f"‚ö†Ô∏è B≈ÇƒÖd API (pr√≥ba {attempt + 1}), ponawiam...")
                    continue
                else:
                    return "Przepraszam, wystƒÖpi≈Ç problem z po≈ÇƒÖczeniem. Spr√≥buj ponownie."
            
            # Sukces!
            return odpowiedz
            
        except Exception as e:
            if attempt == 0:
                print(f"‚ö†Ô∏è B≈ÇƒÖd po≈ÇƒÖczenia (pr√≥ba {attempt + 1}): {e}")
                continue
            else:
                return f"Przepraszam, wystƒÖpi≈Ç b≈ÇƒÖd po≈ÇƒÖczenia: {str(e)}"
    
    return "‚ùå Przekroczono liczbƒô pr√≥b po≈ÇƒÖczenia z LLM"

def zapytaj_llm_safe_with_fallback(tekst, config, max_retries=2):
    """
    Pr√≥buje r√≥≈ºne modele je≈õli g≈Ç√≥wny nie ma kredyt√≥w
    Teraz z obs≈ÇugƒÖ Ollama
    """
    
    provider = config.get("llm_config", {}).get("provider", "openrouter")
    
    # Je≈õli to Ollama - u≈ºyj prostego zapytania
    if provider == "ollama":
        return zapytaj_llm_safe(tekst, config, max_retries)
    
    # OpenRouter - pr√≥buj r√≥≈ºne modele
    modele = [config["llm_config"]["model"]]
    
    if "alternative_models" in config["llm_config"]:
        modele.extend(config["llm_config"]["alternative_models"])
    
    for model in modele:
        print(f"üéØ Pr√≥bujƒô model: {model}")
        
        temp_config = config.copy()
        temp_config["llm_config"]["model"] = model
        
        result = zapytaj_openrouter_safe(tekst, temp_config, max_retries)
        
        if not ("Brak kredyt√≥w API" in result or "can only afford" in result):
            return result
            
        print(f"‚ùå Model {model} wymaga kredyt√≥w, pr√≥bujƒô nastƒôpny...")
    
    return "‚ùå Wszystkie modele wymagajƒÖ do≈Çadowania kredyt√≥w"

# ===================================================================
# ü§ñ SEKCJA 6: LLM INTENT CLASSIFIERS - R√ì≈ªNE METODY
# ===================================================================

def klasyfikuj_intencje_llm_simple(tekst, dostepne_intencje, config):
    """Prosty LLM classifier (stara wersja)"""
    
    intencje_lista = ", ".join([k["intencja"] for k in dostepne_intencje])
    
    prompt_klasyfikacji = f"""UWAGA: Odpowiadaj WY≈ÅƒÑCZNIE po polsku!

Przeanalizuj tekst u≈ºytkownika: "{tekst}"

Dostƒôpne polskie intencje: {intencje_lista}

ZADANIE: Wybierz dok≈Çadnie JEDNƒÑ intencjƒô z listy kt√≥ra pasuje do tekstu.
- Je≈õli tekst m√≥wi o sk≈Çadnikach i gotowaniu ‚Üí "dania_z_skladnikow"  
- Je≈õli pyta o kalorie ‚Üí "kalorie_produktu"
- Je≈õli pyta o godzinƒô ‚Üí "zapytanie_godzina"
- Je≈õli ≈ºadna nie pasuje ‚Üí "brak_dopasowania"

ODPOWIED≈π (TYLKO nazwa intencji po polsku):"""

    try:
       # U≈ºyj tej samej konfiguracji co g≈Ç√≥wny system
        config_klasyfikacja = config.copy()
        config_klasyfikacja["llm_config"]["max_tokens"] = 50
        config_klasyfikacja["llm_config"]["temperature"] = 0.1
        
        print(f"üîç Simple LLM klasyfikuje: '{tekst[:50]}...'")
        odpowiedz = zapytaj_llm_safe(prompt_klasyfikacji, config_klasyfikacja, max_retries=1)
        
        # Wyczy≈õƒá odpowied≈∫
        intencja = odpowiedz.strip().replace("üß† [Czysty LLM]: ", "").strip()
        
        # Sprawd≈∫ czy to prawid≈Çowa intencja
        if intencja in [k["intencja"] for k in dostepne_intencje]:
            print(f"‚úÖ Simple LLM rozpozna≈Ç: {intencja}")
            return intencja
        elif intencja == "brak_dopasowania":
            print(f"‚ùå Simple LLM nie rozpozna≈Ç intencji")
            return None
        else:
            print(f"‚ö†Ô∏è Simple LLM zwr√≥ci≈Ç nieprawid≈ÇowƒÖ intencjƒô: {intencja}")
            return None
            
    except Exception as e:
        print(f"‚ùå B≈ÇƒÖd simple klasyfikacji: {e}")
        return None

def klasyfikuj_intencje_llm_few_shot(tekst, dostepne_intencje, config):
    """Few-shot LLM classifier (ulepszona wersja)"""
    
    # Few-shot examples
    examples = """PRZYK≈ÅADY KLASYFIKACJI:

    "Kt√≥ra godzina?" ‚Üí zapytanie_godzina
    "Kt√≥ra jest godzina?" ‚Üí zapytanie_godzina  
    "Kt√≥rƒÖ mamy godzinƒô?" ‚Üí zapytanie_godzina
    "Powiedz mi godzinƒô" ‚Üí zapytanie_godzina
    "Jaki mamy czas?" ‚Üí zapytanie_godzina

    "Jak siƒô masz?" ‚Üí zapytanie_samopoczucie  
    "Co u ciebie?" ‚Üí zapytanie_samopoczucie
    "Jak leci?" ‚Üí zapytanie_samopoczucie
    "Co s≈Çychaƒá?" ‚Üí zapytanie_samopoczucie
    "Jak tam?" ‚Üí zapytanie_samopoczucie

    "Do zobaczenia!" ‚Üí pozegnanie
    "Na razie" ‚Üí pozegnanie
    "≈ªegnaj" ‚Üí pozegnanie
    "Do widzenia" ‚Üí pozegnanie
    "Papa" ‚Üí pozegnanie

    "Ile kalorii ma jajko?" ‚Üí kalorie_produktu
    "Kalorie w pomidorze?" ‚Üí kalorie_produktu
    "Ile kalorii to ma?" ‚Üí kalorie_produktu

    "Mam jajka, co zrobiƒá?" ‚Üí dania_z_skladnikow
    "Co mo≈ºna z tego ugotowaƒá?" ‚Üí dania_z_skladnikow
    "Co mogƒô przygotowaƒá?" ‚Üí dania_z_skladnikow
    "Mam pomidor, co zrobiƒá?" ‚Üí dania_z_skladnikow

    "Opowiedz dowcip" ‚Üí brak_dopasowania
    "Jak dzia≈Ça samolot?" ‚Üí brak_dopasowania
    "Czy jutro bƒôdzie padaƒá deszcz?" ‚Üí brak_dopasowania"""

    intencje_lista = ", ".join([k["intencja"] for k in dostepne_intencje])
    
    prompt = f"""{examples}

ZADANIE: Przeanalizuj tekst i wybierz najlepszƒÖ intencjƒô.

TEKST: "{tekst}"
DOSTƒòPNE INTENCJE: {intencje_lista}

INSTRUKCJE:
- Je≈õli tekst pasuje do intencji z listy ‚Üí zwr√≥ƒá nazwƒô
- Je≈õli nie pasuje do ≈ºadnej ‚Üí zwr√≥ƒá "brak_dopasowania"  
- Zwr√≥ƒá TYLKO nazwƒô intencji

ODPOWIED≈π:"""

    try:
        config_klasyfikacja = {
            "llm_config": {
                "provider": "openrouter",  # Zawsze OpenRouter dla klasyfikacji
                "model": "openai/gpt-3.5-turbo",
                "max_tokens": 50,
                "temperature": 0.1
            }
        }
        
        print(f"üîç Few-shot LLM klasyfikuje: '{tekst[:50]}...'")
        odpowiedz = zapytaj_llm_safe(prompt, config_klasyfikacja, max_retries=1)
        
        # Wyczy≈õƒá odpowied≈∫
        intencja = odpowiedz.strip().replace("üß† [Czysty LLM]: ", "").strip()
        
        # Sprawd≈∫ czy to prawid≈Çowa intencja
        if intencja in [k["intencja"] for k in dostepne_intencje]:
            print(f"‚úÖ Few-shot LLM rozpozna≈Ç: {intencja}")
            return intencja
        elif intencja == "brak_dopasowania":
            print(f"‚ùå Few-shot LLM nie rozpozna≈Ç intencji")
            return None
        else:
            print(f"‚ö†Ô∏è Few-shot LLM zwr√≥ci≈Ç nieprawid≈ÇowƒÖ intencjƒô: {intencja}")
            return None
            
    except Exception as e:
        print(f"‚ùå B≈ÇƒÖd few-shot klasyfikacji: {e}")
        return None

# ===================================================================
# üéØ SEKCJA 7: G≈Å√ìWNA LOGIKA ANALIZY - SWITCHER SYSTEM
# ===================================================================

def analizuj(tekst, config, tts_module):
    """
    G≈Ç√≥wna funkcja analizy tekstu z switcherem metod rozpoznawania
    
    Nowy system:
    1. IntentRecognizer wybiera metodƒô z config
    2. R√≥≈ºne metody: regex_only, regex_plus_simple, regex_plus_few_shot
    3. Fallback na czysty LLM je≈õli nic nie pasuje
    4. Obs≈Çuga Ollama + OpenRouter
    
    Args:
        tekst (str): Tekst u≈ºytkownika
        config (dict): Konfiguracja systemu
        tts_module: Modu≈Ç Text-to-Speech
        
    Returns:
        str: Odpowied≈∫ systemu
    """
    start_time = time.time()
    intencja = None
    model_llm = None
    odpowiedz = ""
    
    print("üì• Otrzymano polecenie:", tekst)

    try:
        # ===============================================================
        # NOWY SYSTEM: SWITCHER METOD ROZPOZNAWANIA
        # ===============================================================
        
        # Inicjalizuj recognizer z config
        global intent_recognizer
        intent_recognizer = IntentRecognizer(config)
        
        # U≈ºyj wybranej metody klasyfikacji
        intencja, method_used = intent_recognizer.classify_intent(tekst, KOMENDY, config)
        
        if intencja:
            print(f"‚úÖ Rozpoznano intencjƒô ({method_used}): {intencja}")
            
            # Loguj rozpoznanƒÖ intencjƒô
            logger.loguj_intencje(intencja, tekst)
            
            # Wykonaj intencjƒô
            odpowiedz = wykonaj_intencje(intencja, tekst, tts_module, config)
            
            # Sprawd≈∫ czy to by≈Ça implementacja lokalna czy LLM fallback
            if "ü§ñ [LLM za intencjƒô" in odpowiedz:
                # To by≈Ç LLM fallback - ju≈º zalogowane w wykonaj_intencje
                return odpowiedz
            else:
                # Lokalna implementacja
                logger.loguj_rozmowe(
                    tekst_wej=tekst,
                    tekst_wyj=odpowiedz,
                    intencja=intencja,
                    model_llm=None,
                    czas_start=start_time,
                    metadata={"typ": f"intencja_{method_used}", "method": method_used}
                )
            
            return odpowiedz

        # ===============================================================
        # POZIOM 3: CZYSTY LLM - OG√ìLNA ROZMOWA
        # ===============================================================
        print("ü§ñ ≈ªadna metoda nie rozpozna≈Ça intencji ‚Äì pytam LLM...")
        
        try:
            model_llm = config["llm_config"]["model"]
            provider = config.get("llm_config", {}).get("provider", "openrouter")
            
            # U≈ºyj bezpiecznej funkcji LLM z fallback
            polish_prompt = f"Odpowiadaj TYLKO po polsku. U≈ºytkownik powiedzia≈Ç: '{tekst}'"
            odpowiedz = zapytaj_llm_safe_with_fallback(polish_prompt, config)
            
            # Dodaj prefix ≈ºeby by≈Ço widaƒá ≈ºe to czysty LLM
            if not odpowiedz.startswith("‚ùå"):
                if provider == "ollama":
                    odpowiedz = f"üß† [Ollama LLM]: {odpowiedz}"
                else:
                    odpowiedz = f"üß† [Czysty LLM]: {odpowiedz}"
            
            print(f"üß† LLM odpowiada: {odpowiedz}")
            
            # Sprawd≈∫ czy odpowied≈∫ nie jest b≈Çƒôdem
            if odpowiedz.startswith("‚ùå") or odpowiedz.startswith("[B≈ÇƒÖd"):
                logger.loguj_blad("llm_error", odpowiedz, {"tekst": tekst, "model": model_llm})
                if "Brak kredyt√≥w API" in odpowiedz:
                    # Zachowaj oryginalnƒÖ wiadomo≈õƒá o brakach kredyt√≥w
                    pass  # nie zmieniaj odpowiedzi
                else:
                    odpowiedz = "Przepraszam, wystƒÖpi≈Ç problem z po≈ÇƒÖczeniem. Spr√≥buj ponownie."
            
            # NAPRAWIONE: Wypowiedz odpowied≈∫ BEZ prefix√≥w
            tts_module.mow_tekstem(wypowiedz_bez_prefixow(odpowiedz))
            
            # Loguj rozmowƒô LLM
            logger.loguj_rozmowe(
                tekst_wej=tekst,
                tekst_wyj=odpowiedz,
                intencja=None,
                model_llm=model_llm,
                czas_start=start_time,
                metadata={"typ": "llm_response", "provider": provider}
            )
            
        except Exception as e:
            error_msg = f"Przepraszam, wystƒÖpi≈Ç b≈ÇƒÖd podczas przetwarzania: {str(e)}"
            logger.loguj_blad("llm_processing_error", str(e), {"tekst": tekst})
            
            tts_module.mow_tekstem(error_msg)
            
            # Loguj b≈ÇƒÖd jako rozmowƒô
            logger.loguj_rozmowe(
                tekst_wej=tekst,
                tekst_wyj=error_msg,
                intencja=None,
                model_llm=model_llm,
                czas_start=start_time,
                metadata={"typ": "error_response", "error": str(e)}
            )
            
            odpowiedz = error_msg
        
    except Exception as e:
        error_msg = f"WystƒÖpi≈Ç nieoczekiwany b≈ÇƒÖd systemu."
        logger.loguj_blad("system_error", str(e), {"tekst": tekst, "function": "analizuj"})
        
        tts_module.mow_tekstem(error_msg)
        odpowiedz = error_msg
    
    return odpowiedz

# ===================================================================
# üç≥ SEKCJA 8: FUNKCJE RAG - SYSTEM PRZEPIS√ìW KULINARNYCH
# ===================================================================

def wyciagnij_skladniki(tekst):
    """
    WyciƒÖga sk≈Çadniki z tekstu u≈ºytkownika
    
    Przeszukuje tekst pod kƒÖtem znanych sk≈Çadnik√≥w z bazy.
    U≈ºywa prostego dopasowania string√≥w (case-insensitive).
    
    Args:
        tekst (str): Tekst u≈ºytkownika
        
    Returns:
        list: Lista znalezionych sk≈Çadnik√≥w
    """
    # Lista mo≈ºliwych sk≈Çadnik√≥w (z bazy danych)
    mozliwe_skladniki = [
        "awokado", "banan", "broku≈Çy", "cebula", "chleb", "cukinia", "cytryna", 
        "czosnek", "imbir", "jajka", "jarmu≈º", "kapary", "kasza", "koperek",
        "kurczak", "marchewka", "mleko", "ocet", "og√≥rek", "oliwa", "papryka",
        "pieprz", "pomidor", "soczewica", "sos", "szpinak", "s≈Çonecznik", "tofu", "≈Çoso≈õ"
    ]
    
    znalezione = []
    tekst_lower = tekst.lower()
    
    for skladnik in mozliwe_skladniki:
        if skladnik in tekst_lower:
            znalezione.append(skladnik)
    
    return znalezione

def obs≈Çuga_rag_ogolna(tekst, intencja):
    """
    Obs≈Çuga og√≥lnych zapyta≈Ñ RAG - wyszukiwanie przepis√≥w
    
    Proces:
    1. WyciƒÖgnij sk≈Çadniki z tekstu
    2. Wyszukaj przepisy w RAG engine
    3. Sformu≈Çuj odpowied≈∫ z kategoryzacjƒÖ
    
    Args:
        tekst (str): Tekst u≈ºytkownika
        intencja (str): Rozpoznana intencja
        
    Returns:
        str: Odpowied≈∫ z propozycjami przepis√≥w
    """
    if not RAG_AVAILABLE:
        return "Przepraszam, system przepis√≥w jest chwilowo niedostƒôpny."
    
    # WyciƒÖgnij sk≈Çadniki z tekstu
    skladniki = wyciagnij_skladniki(tekst)
    
    if not skladniki:
        return "Nie rozpozna≈Çem ≈ºadnych sk≈Çadnik√≥w. Spr√≥buj: 'mam jajka, paprykƒô, pomidory'"
    
    print(f"üîç Znalezione sk≈Çadniki: {skladniki}")
    
    # Wyszukaj przepisy
    result = recipe_rag.suggest_recipes(skladniki, max_results=3)
    
    if result.get('found_recipes', 0) == 0:
        return f"Nie znalaz≈Çem przepis√≥w z {', '.join(skladniki)}. Spr√≥buj innych sk≈Çadnik√≥w."
    
    # Sformu≈Çuj odpowied≈∫
    odpowiedz = f"Z {', '.join(skladniki)} mogƒô zaproponowaƒá: "
    
    for category, recipes in result.get('by_category', {}).items():
        cat_name = {"wege": "wegetaria≈Ñskie", "niskot≈Çuszczowa": "niskot≈Çuszczowe", 
                   "niskocukrowa": "niskocukrowe", "keto": "keto"}.get(category, category)
        
        recipe_names = [r['title'] for r in recipes[:2]]  # max 2 z kategorii
        odpowiedz += f"{cat_name}: {', '.join(recipe_names)}. "
    
    return odpowiedz

def obs≈Çuga_rag_kategoria(tekst, kategoria):
    """
    Obs≈Çuga zapyta≈Ñ z konkretnƒÖ kategoriƒÖ diety
    
    Obs≈Çuguje kategorie: wege, niskot≈Çuszczowa, niskocukrowa, keto
    
    Args:
        tekst (str): Tekst u≈ºytkownika
        kategoria (str): Kategoria diety
        
    Returns:
        str: Odpowied≈∫ z przepisami z danej kategorii
    """
    if not RAG_AVAILABLE:
        return "Przepraszam, system przepis√≥w jest chwilowo niedostƒôpny."
    
    # WyciƒÖgnij sk≈Çadniki je≈õli sƒÖ
    skladniki = wyciagnij_skladniki(tekst)
    
    if not skladniki:
        # Brak sk≈Çadnik√≥w - poka≈º og√≥lne przepisy z kategorii
        result = recipe_rag.suggest_recipes([''], category=kategoria, max_results=3)
    else:
        # Z konkretnymi sk≈Çadnikami
        result = recipe_rag.suggest_recipes(skladniki, category=kategoria, max_results=3)
    
    if result.get('found_recipes', 0) == 0:
        cat_name = {"wege": "wegetaria≈Ñskich", "niskot≈Çuszczowa": "niskot≈Çuszczowych", 
                   "niskocukrowa": "niskocukrowych"}.get(kategoria, kategoria)
        return f"Nie znalaz≈Çem {cat_name} przepis√≥w. Spr√≥buj innych sk≈Çadnik√≥w."
    
    # Sformu≈Çuj odpowied≈∫
    cat_name = {"wege": "wegetaria≈Ñskie", "niskot≈Çuszczowa": "niskot≈Çuszczowe", 
               "niskocukrowa": "niskocukrowe"}.get(kategoria, kategoria)
    
    recipes = result.get('all_recipes', [])
    recipe_names = [r['title'] for r in recipes[:3]]
    
    if skladniki:
        odpowiedz = f"Dania {cat_name} z {', '.join(skladniki)}: {', '.join(recipe_names)}"
    else:
        odpowiedz = f"Proponujƒô {cat_name} dania: {', '.join(recipe_names)}"
    
    return odpowiedz

# ===================================================================
# üî¢ SEKCJA 8.1: SYSTEM KALORII - OBLICZENIA WARTO≈öCI OD≈ªYWCZYCH
# ===================================================================

def oblicz_kalorie_produktu(tekst):
    """
    WyciƒÖga produkt z tekstu i zwraca kalorie z bazy sk≈Çadnik√≥w
    
    Regex pattern: "ile.*kalorii.*ma.*produkt"
    ≈πr√≥d≈Ço danych: recipe_rag.engine.loader.skladniki
    
    Args:
        tekst (str): Tekst u≈ºytkownika z pytaniem o kalorie
        
    Returns:
        str: Informacja o kaloriach produktu
    """
    import re
    match = re.search(r'ile.*kalorii.*ma.*?([a-zA-ZƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈ºƒÑƒÜƒò≈Å≈É√ì≈ö≈π≈ª]+)', tekst, re.IGNORECASE)
    
    if not match:
        return "Nie zrozumia≈Çem o jaki produkt pytasz."
    
    produkt = match.group(1).lower()
    print(f"üîç Szukam kalorii dla: {produkt}")
    
    # Sprawd≈∫ w bazie sk≈Çadnik√≥w
    if RAG_AVAILABLE and recipe_rag:
        try:
            skladniki = recipe_rag.engine.loader.skladniki
            if produkt in skladniki:
                kalorie = skladniki[produkt]['kalorie_na_100g']
                waga = skladniki[produkt]['waga_standardowa'] 
                jednostka = skladniki[produkt]['jednostka']
                return f"{produkt.capitalize()} ma {kalorie} kalorii na 100g. Standardowa porcja ({waga}g/{jednostka}) to {int(kalorie * waga / 100)} kalorii."
            else:
                return f"Nie mam informacji o kaloriach dla {produkt}. Dostƒôpne produkty: jajka, papryka, pomidor, ≈Çoso≈õ, tofu..."
        except Exception as e:
            print(f"‚ùå B≈ÇƒÖd sprawdzania kalorii: {e}")
            return "WystƒÖpi≈Ç b≈ÇƒÖd podczas sprawdzania kalorii."
    else:
        return "System kalorii jest niedostƒôpny."

# ===================================================================
# üé≠ SEKCJA 9: WYKONANIE INTENCJI - LOKALNE IMPLEMENTACJE
# ===================================================================

def wykonaj_intencje(intencja, tekst, tts_module, config):
    """
    Wykonuje konkretnƒÖ intencjƒô z pe≈Çnym logowaniem
    
    Obs≈Çuguje:
    - Intencje podstawowe (godzina, data, powitania)
    - Intencje systemowe (status, pamiƒôƒá)  
    - Intencje kulinarne (RAG)
    - Fallback na LLM dla nieznanych intencji
    
    Args:
        intencja (str): Nazwa intencji do wykonania
        tekst (str): Oryginalny tekst u≈ºytkownika
        tts_module: Modu≈Ç Text-to-Speech
        config (dict): Konfiguracja systemu
        
    Returns:
        str: Odpowied≈∫ systemu
    """
    odpowiedz = ""
    
    try:
        # ===============================================================
        # GRUPA 9.1: INTENCJE PODSTAWOWE - CZAS I POWITANIA
        # ===============================================================
        if intencja == "zapytanie_godzina":
            from datetime import datetime
            godzina = datetime.now().strftime("%H:%M")
            odpowiedz = f"Jest godzina {godzina}"
            
        elif intencja == "zapytanie_data":
            from datetime import datetime
            data = datetime.now().strftime("%d %B %Y")
            odpowiedz = f"Dzisiaj jest {data}"
            
        elif intencja == "powitanie":
            import random
            powitania = [
                "Cze≈õƒá! Jak mogƒô pom√≥c?",
                "Witaj! W czym mogƒô Ci pom√≥c?",
                "Hej! Gotowy do pracy!"
            ]
            odpowiedz = random.choice(powitania)
            
        elif intencja == "pozegnanie":
            import random
            pozegnania = [
                "Do widzenia!",
                "Mi≈Çego dnia!",
                "Do zobaczenia!"
            ]
            odpowiedz = random.choice(pozegnania)
            
        elif intencja == "zapytanie_samopoczucie":
            import random
            samopoczucie = [
                "Dziƒôki za pytanie! Wszystko w porzƒÖdku!",
                "≈öwietnie siƒô mam! Gotowy do pomocy!",
                "Doskonale! Co mogƒô dla Ciebie zrobiƒá?"
            ]
            odpowiedz = random.choice(samopoczucie)
            
        # ===============================================================
        # GRUPA 9.2: INTENCJE SYSTEMOWE - STATUS I DIAGNOSTYKA
        # ===============================================================
        elif intencja == "status_systemu":
            import torch
            gpu_status = "aktywne" if torch.cuda.is_available() else "nieaktywne"
            rag_status = "dostƒôpny" if RAG_AVAILABLE else "niedostƒôpny"
            ollama_status = "dostƒôpny" if OLLAMA_AVAILABLE else "niedostƒôpny"
            
            # Dodaj info o limitach token√≥w i metodzie rozpoznawania
            cached_models = len(token_manager.model_limits)
            method = intent_recognizer.method if intent_recognizer else "unknown"
            provider = config.get("llm_config", {}).get("provider", "openrouter")
            model = config.get("llm_config", {}).get("model", "unknown")
            
            odpowiedz = f"System AIA dzia≈Ça poprawnie. GPU: {gpu_status}, RAG: {rag_status}, Ollama: {ollama_status}, Provider: {provider}, Model: {model}, Metoda: {method}, Cache token√≥w: {cached_models} modeli"
            
        # ===============================================================
        # GRUPA 9.3: INTENCJE PAMIƒòCI - NOTATKI I WIADOMO≈öCI
        # ===============================================================
        elif intencja == "zapisz_wiadomosc":
            # WyciƒÖgnij tre≈õƒá wiadomo≈õci z tekstu
            import re
            match = re.search(r'(?:zapisz|dodaj|utw[o√≥]rz).*wiadomo[≈õƒás][ƒác]?\s*[":]\s*(.+)', tekst, re.IGNORECASE)
            if match:
                tresc = match.group(1).strip()
                from core.pamiec import zapisz_wiadomosc
                uuid_msg = zapisz_wiadomosc("Notatka g≈Çosowa", tresc, "AIA U≈ºytkownik")
                odpowiedz = f"Zapisa≈Çem wiadomo≈õƒá: {tresc}"
            else:
                odpowiedz = "Nie zrozumia≈Çem co mam zapisaƒá. Spr√≥buj: 'Zapisz wiadomo≈õƒá: tre≈õƒá'"
                
        elif intencja == "odczytaj_wiadomosc":
            from core.pamiec import pobierz_wiadomosci
            wiadomosci = pobierz_wiadomosci(limit=3)
            if wiadomosci:
                odpowiedz = "Twoje ostatnie wiadomo≈õci: "
                for i, w in enumerate(wiadomosci, 1):
                    odpowiedz += f"{i}. {w['tytul']}: {w['tresc']}. "
            else:
                odpowiedz = "Nie masz ≈ºadnych zapisanych wiadomo≈õci."
                
        elif intencja == "przeglad_wiadomosci":
            from core.pamiec import pobierz_wiadomosci
            wiadomosci = pobierz_wiadomosci(limit=5)
            if wiadomosci:
                odpowiedz = f"Masz {len(wiadomosci)} wiadomo≈õci. "
                for i, w in enumerate(wiadomosci, 1):
                    odpowiedz += f"{i}. {w['tytul']} z {w['timestamp'][:10]}. "
            else:
                odpowiedz = "Nie masz ≈ºadnych zapisanych wiadomo≈õci."
        
        # ===============================================================
        # GRUPA 9.4: INTENCJE KULINARNE - RAG SYSTEM
        # ===============================================================
        elif intencja in ["co_moge_zrobic_z_lodowki", "zaproponuj_dania", "dania_z_skladnikow"]:
            odpowiedz = obs≈Çuga_rag_ogolna(tekst, intencja)
            
        elif intencja == "dania_wege":
            odpowiedz = obs≈Çuga_rag_kategoria(tekst, "wege")
            
        elif intencja == "dania_niskotluszczowe":
            odpowiedz = obs≈Çuga_rag_kategoria(tekst, "niskot≈Çuszczowa")
            
        elif intencja == "dania_niskocukrowe":
            odpowiedz = obs≈Çuga_rag_kategoria(tekst, "niskocukrowa")
            
        elif intencja == "kalorie_produktu":
            odpowiedz = oblicz_kalorie_produktu(tekst)
            
        # ===============================================================
        # GRUPA 9.5: INTENCJE PLACEHOLDER - DO IMPLEMENTACJI
        # ===============================================================
        elif intencja == "kalorie_przepisu":
            odpowiedz = "Funkcja liczenia kalorii bƒôdzie wkr√≥tce dostƒôpna."
            
        elif intencja == "przepis_szczegolowy":
            odpowiedz = "Szczeg√≥≈Çowe przepisy bƒôdƒÖ dostƒôpne w przysz≈Çej wersji."
            
        elif intencja == "skladniki_na_danie":
            odpowiedz = "Funkcja wy≈õwietlania sk≈Çadnik√≥w bƒôdzie wkr√≥tce dostƒôpna."
            
        # ===============================================================
        # GRUPA 9.6: FALLBACK LLM - NIEZNANE INTENCJE
        # ===============================================================
        else:
            # Nieznana intencja ‚Üí przeka≈º do LLM
            print(f"‚ùì Nieznana intencja '{intencja}' - przekazujƒô do LLM...")
            
            try:
                model_llm = "LLM_fallback"  # Oznacz ≈ºe to fallback
                provider = config.get("llm_config", {}).get("provider", "openrouter")
                odpowiedz_llm = zapytaj_llm_safe_with_fallback(tekst, config)
                
                # Dodaj prefix ≈ºeby by≈Ço widaƒá ≈∫r√≥d≈Ço
                if provider == "ollama":
                    odpowiedz = f"ü§ñ [Ollama za intencjƒô '{intencja}']: {odpowiedz_llm}"
                else:
                    odpowiedz = f"ü§ñ [LLM za intencjƒô '{intencja}']: {odpowiedz_llm}"
                
                # Zaloguj jako hybrydowƒÖ odpowied≈∫
                logger.loguj_rozmowe(
                    tekst_wej=tekst,
                    tekst_wyj=odpowiedz,
                    intencja=intencja,
                    model_llm=model_llm,
                    czas_start=time.time(),
                    metadata={"typ": "intencja_llm_fallback", "original_intent": intencja, "provider": provider}
                )
                
            except Exception as e:
                odpowiedz = f"‚ùå Zrozumia≈Çem intencjƒô '{intencja}', ale wystƒÖpi≈Ç b≈ÇƒÖd podczas przetwarzania: {str(e)}"
                logger.loguj_blad("intent_llm_fallback_error", str(e), {"intencja": intencja, "tekst": tekst})
            
    except Exception as e:
        logger.loguj_blad("intencja_error", f"B≈ÇƒÖd podczas wykonywania intencji {intencja}: {e}", {"tekst": tekst})
        odpowiedz = "Przepraszam, wystƒÖpi≈Ç b≈ÇƒÖd podczas wykonywania polecenia."

    # ===================================================================
    # SEKCJA 9.7: FINALIZACJA - TTS I RETURN
    # ===================================================================
    
    # Wy≈õwietl i wypowiedz odpowied≈∫
    print(f"üó£Ô∏è AIA odpowiada: {odpowiedz}")
    try:
        # NAPRAWIONE: Usu≈Ñ prefixy przed TTS
        tts_module.mow_tekstem(wypowiedz_bez_prefixow(odpowiedz))
    except Exception as e:
        logger.loguj_blad("tts_error", f"B≈ÇƒÖd TTS: {e}", {"odpowiedz": odpowiedz})

    return odpowiedz

# ===================================================================
# üõ†Ô∏è SEKCJA 10: FUNKCJE POMOCNICZE - ZARZƒÑDZANIE I DIAGNOSTYKA
# ===================================================================

def dodaj_komende(wzorzec, intencja):
    """
    Dodaje nowƒÖ komendƒô do listy (w runtime)
    
    Args:
        wzorzec (str): Wzorzec regex
        intencja (str): Nazwa intencji
    """
    KOMENDY.append({"wzorzec": wzorzec, "intencja": intencja})
    print(f"‚úÖ Dodano komendƒô: {wzorzec} -> {intencja}")

def lista_intencji():
    """
    Zwraca listƒô wszystkich dostƒôpnych intencji
    
    Returns:
        list: Lista nazw intencji
    """
    return [komenda["intencja"] for komenda in KOMENDY]

def sprawdz_limity_tokenow():
    """
    Zwraca informacje o zapisanych limitach token√≥w
    
    Returns:
        dict: S≈Çownik z limitami per model
    """
    return token_manager.model_limits

def wyczysc_cache_tokenow():
    """
    Czy≈õci cache limit√≥w token√≥w
    """
    token_manager.model_limits = {}
    token_manager.save_cache()
    print("üßπ Wyczyszczono cache limit√≥w token√≥w")

def get_recognition_stats():
    """
    Zwraca statystyki rozpoznawania
    
    Returns:
        dict: Statystyki aktualnej sesji
    """
    if intent_recognizer:
        return {
            "method": intent_recognizer.method,
            "confidence_threshold": intent_recognizer.confidence_threshold,
            "use_context": intent_recognizer.use_context,
            "debug_mode": intent_recognizer.debug_mode
        }
    return {"status": "not_initialized"}

def get_llm_stats():
    """
    Zwraca statystyki LLM i provider
    
    Returns:
        dict: Informacje o aktualnej konfiguracji LLM
    """
    return {
        "ollama_available": OLLAMA_AVAILABLE,
        "token_limits_cached": len(token_manager.model_limits),
        "cache_file": token_manager.cache_file
    }

# ===================================================================
# üß™ SEKCJA 11: TESTY LOKALNE - ROZW√ìJ I DIAGNOSTYKA
# ===================================================================

if __name__ == "__main__":
    print("üß™ Test modu≈Çu rozumienie z obs≈ÇugƒÖ Ollama")
    
    # Mock config i TTS dla test√≥w
    mock_config_openrouter = {
        "local_config": {"styl": "precyzyjny"},
        "llm_config": {
            "provider": "openrouter",
            "model": "openai/gpt-3.5-turbo", 
            "max_tokens": 2048
        },
        "recognition_config": {
            "method": "regex_plus_few_shot",
            "confidence_threshold": 0.7,
            "debug_mode": True
        }
    }
    
    mock_config_ollama = {
        "local_config": {"styl": "precyzyjny"},
        "llm_config": {
            "provider": "ollama",
            "model": "llama3.1:8b",
            "base_url": "http://localhost:11434",
            "max_tokens": 2048,
            "temperature": 0.7
        },
        "recognition_config": {
            "method": "regex_plus_simple",
            "confidence_threshold": 0.7,
            "debug_mode": True
        }
    }
    
    class MockTTS:
        def mow_tekstem(self, tekst):
            print(f"[MOCK TTS]: {tekst}")
    
    mock_tts = MockTTS()
    
    # Testy poszczeg√≥lnych komponent√≥w
    print("\n--- Test 1: Zapytanie o godzinƒô (regex) ---")
    analizuj("kt√≥ra godzina?", mock_config_ollama, mock_tts)
    
    print("\n--- Test 2: Status systemu ---") 
    analizuj("status systemu", mock_config_ollama, mock_tts)
    
    print("\n--- Test 3: Test Ollama (je≈õli dostƒôpny) ---")
    if OLLAMA_AVAILABLE:
        analizuj("powiedz kr√≥tko cze≈õƒá", mock_config_ollama, mock_tts)
    else:
        print("Ollama niedostƒôpny - pomijam test")
    
    print("\n--- Test 4: Statystyki ---")
    print("Recognition Stats:", get_recognition_stats())
    print("LLM Stats:", get_llm_stats())

# ===================================================================
# KONIEC PLIKU - CORE/ROZUMIENIE.PY Z OBS≈ÅUGƒÑ OLLAMA
# ===================================================================