# core/rozumienie.py
import json
import re
import time
import os
from llm import llm_openrouter
from core import logger

# === NOWY - Token Manager ===
class TokenManager:
    def __init__(self):
        self.cache_file = "data/token_limits.json"
        self.model_limits = self.load_cache()
        self.default_safe_tokens = 150  # Bezpieczny start
        
    def load_cache(self):
        """Wczytuje zapisane limity tokenÃ³w z pliku"""
        try:
            if os.path.exists(self.cache_file):
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            print(f"âš ï¸ Nie moÅ¼na wczytaÄ‡ cache tokenÃ³w: {e}")
        return {}
    
    def save_cache(self):
        """Zapisuje limity tokenÃ³w do pliku"""
        try:
            os.makedirs(os.path.dirname(self.cache_file), exist_ok=True)
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.model_limits, f, indent=2)
        except Exception as e:
            print(f"âš ï¸ Nie moÅ¼na zapisaÄ‡ cache tokenÃ³w: {e}")
    
    def get_safe_tokens(self, model, requested_tokens=2048):
        """Zwraca bezpiecznÄ… liczbÄ™ tokenÃ³w dla modelu"""
        if model in self.model_limits:
            safe_limit = min(requested_tokens, self.model_limits[model])
            print(f"ğŸ”’ Model {model}: uÅ¼ywam zapisanego limitu {safe_limit} tokenÃ³w")
            return safe_limit
        else:
            print(f"ğŸ†• Model {model}: pierwsza prÃ³ba z {self.default_safe_tokens} tokenÃ³w")
            return min(requested_tokens, self.default_safe_tokens)
    
    def handle_402_error(self, model, error_message):
        """WyciÄ…ga i zapisuje rzeczywisty limit z bÅ‚Ä™du 402"""
        match = re.search(r"can only afford (\d+)", error_message)
        if match:
            actual_limit = int(match.group(1))
            self.model_limits[model] = actual_limit
            self.save_cache()
            print(f"ğŸ’¾ Zapisano limit dla {model}: {actual_limit} tokenÃ³w")
            return actual_limit
        return self.default_safe_tokens

# Globalna instancja Token Manager
token_manager = TokenManager()

# === NOWE - Import RAG ===
try:
    import sys
    import os
    rag_path = os.path.join(os.path.dirname(__file__), 'rag')
    sys.path.insert(0, rag_path)
    from rag_engine import RecipeRAG
    RAG_AVAILABLE = True
    # Inicjalizacja RAG - raz przy starcie
    recipe_rag = RecipeRAG()
    if recipe_rag.initialize():
        print("âœ… RAG zainicjalizowany w rozumieniu")
    else:
        print("âŒ BÅ‚Ä…d inicjalizacji RAG")
        RAG_AVAILABLE = False
except Exception as e:
    print(f"âš ï¸ RAG niedostÄ™pny: {e}")
    RAG_AVAILABLE = False
    recipe_rag = None

# === 1. Wczytanie komend predefiniowanych ===
try:
    with open("config/komendy_domyslne.json", encoding="utf-8") as f:
        KOMENDY = json.load(f)
    print(f"âœ… Wczytano {len(KOMENDY)} predefiniowanych komend")
except FileNotFoundError:
    print("âš ï¸ Brak pliku komendy_domyslne.json - uÅ¼ywam trybu tylko LLM")
    KOMENDY = []
except json.JSONDecodeError as e:
    print(f"âŒ BÅ‚Ä…d w pliku komendy_domyslne.json: {e}")
    KOMENDY = []

# === 2.5. NOWA FUNKCJA - LLM Intent Classifier ===
def klasyfikuj_intencje_llm(tekst, dostepne_intencje):
    """UÅ¼ywa LLM do inteligentnej klasyfikacji intencji"""
    
    # Przygotuj prompt z listÄ… dostÄ™pnych intencji
    intencje_lista = ", ".join([k["intencja"] for k in dostepne_intencje])
    
    prompt_klasyfikacji = f"""Przeanalizuj tekst uÅ¼ytkownika i okreÅ›l jego intencjÄ™.

TEKST: "{tekst}"

DOSTÄ˜PNE INTENCJE:
{intencje_lista}

Zadanie: Wybierz DOKÅADNIE JEDNÄ„ intencjÄ™ z listy powyÅ¼ej, ktÃ³ra najlepiej pasuje do tekstu.
JeÅ›li Å¼adna nie pasuje, odpowiedz: "brak_dopasowania"

Odpowiedz TYLKO nazwÄ… intencji, bez dodatkowych sÅ‚Ã³w."""

    try:
        # UÅ¼yj maÅ‚ego, szybkiego modelu do klasyfikacji
        config_klasyfikacja = {
            "llm_config": {
                "model": "openai/gpt-3.5-turbo",  # Szybki i tani
                "max_tokens": 50,
                "temperature": 0.1  # Deterministyczne odpowiedzi
            }
        }
        
        print(f"ğŸ” LLM klasyfikuje intencjÄ™ dla: '{tekst[:50]}...'")
        odpowiedz = zapytaj_llm_safe(prompt_klasyfikacji, config_klasyfikacja, max_retries=1)
        
        # WyczyÅ›Ä‡ odpowiedÅº
        intencja = odpowiedz.strip().replace("ğŸ§  [Czysty LLM]: ", "").strip()
        
        # SprawdÅº czy to prawidÅ‚owa intencja
        if intencja in [k["intencja"] for k in dostepne_intencje]:
            print(f"âœ… LLM rozpoznaÅ‚ intencjÄ™: {intencja}")
            return intencja
        elif intencja == "brak_dopasowania":
            print(f"âŒ LLM nie rozpoznaÅ‚ Å¼adnej intencji")
            return None
        else:
            print(f"âš ï¸ LLM zwrÃ³ciÅ‚ nieprawidÅ‚owÄ… intencjÄ™: {intencja}")
            return None
            
    except Exception as e:
        print(f"âŒ BÅ‚Ä…d klasyfikacji LLM: {e}")
        return None
def zapytaj_llm_safe_with_fallback(tekst, config, max_retries=2):
    """PrÃ³buje rÃ³Å¼ne modele jeÅ›li gÅ‚Ã³wny nie ma kredytÃ³w"""
    
    modele = [config["llm_config"]["model"]]  # GÅ‚Ã³wny model
    
    # Dodaj alternatywne modele jeÅ›li sÄ…
    if "alternative_models" in config["llm_config"]:
        modele.extend(config["llm_config"]["alternative_models"])
    
    for model in modele:
        print(f"ğŸ¯ PrÃ³bujÄ™ model: {model}")
        
        # Skopiuj config z nowym modelem
        temp_config = config.copy()
        temp_config["llm_config"]["model"] = model
        
        result = zapytaj_llm_safe(tekst, temp_config, max_retries)
        
        # JeÅ›li sukces lub nie ma bÅ‚Ä™du kredytÃ³w
        if not ("Brak kredytÃ³w API" in result or "can only afford" in result):
            return result
            
        print(f"âŒ Model {model} wymaga kredytÃ³w, prÃ³bujÄ™ nastÄ™pny...")
    
    return "âŒ Wszystkie modele wymagajÄ… doÅ‚adowania kredytÃ³w"

def zapytaj_llm_safe(tekst, config, max_retries=2):
    """Bezpieczne zapytanie LLM z auto-adjustem tokenÃ³w"""
    
    model = config["llm_config"]["model"]
    requested_tokens = config["llm_config"].get("max_tokens", 2048)
    
    for attempt in range(max_retries):
        # Pobierz bezpiecznÄ… liczbÄ™ tokenÃ³w
        safe_tokens = token_manager.get_safe_tokens(model, requested_tokens)
        
        # Aktualizuj config z bezpiecznymi tokenami
        temp_config = config.copy()
        temp_config["llm_config"]["max_tokens"] = safe_tokens
        
        try:
            print(f"ğŸ§  Pytam {model} (tokens: {safe_tokens})...")
            odpowiedz = llm_openrouter.odpowiedz(tekst, temp_config)
            
            # SprawdÅº czy odpowiedÅº zawiera bÅ‚Ä…d 402
            if "[BÅ‚Ä…d API OpenRouter: 402]" in odpowiedz and "can only afford" in odpowiedz:
                # WyciÄ…gnij liczbÄ™ tokenÃ³w z bÅ‚Ä™du
                actual_limit = token_manager.handle_402_error(model, odpowiedz)
                
                print(f"ğŸ”„ Wykryto limit {actual_limit} tokenÃ³w, ponawiam zapytanie...")
                
                # JeÅ›li to pierwsza prÃ³ba, sprÃ³buj ponownie
                if attempt == 0:
                    continue
                else:
                    return f"âŒ Brak kredytÃ³w API. DostÄ™pne tokeny: {actual_limit}. DoÅ‚aduj konto na https://openrouter.ai/settings/credits"
            
            elif odpowiedz.startswith("[BÅ‚Ä…d"):
                # Inny bÅ‚Ä…d API
                if attempt == 0:
                    print(f"âš ï¸ BÅ‚Ä…d API (prÃ³ba {attempt + 1}), ponawiam...")
                    continue
                else:
                    return "Przepraszam, wystÄ…piÅ‚ problem z poÅ‚Ä…czeniem. SprÃ³buj ponownie."
            
            # Sukces!
            return odpowiedz
            
        except Exception as e:
            if attempt == 0:
                print(f"âš ï¸ BÅ‚Ä…d poÅ‚Ä…czenia (prÃ³ba {attempt + 1}): {e}")
                continue
            else:
                return f"Przepraszam, wystÄ…piÅ‚ bÅ‚Ä…d poÅ‚Ä…czenia: {str(e)}"
    
    return "âŒ Przekroczono liczbÄ™ prÃ³b poÅ‚Ä…czenia z LLM"

# === 3. Analiza tekstu (zaktualizowana) ===
def analizuj(tekst, config, tts_module):
    """
    GÅ‚Ã³wna funkcja analizy tekstu z peÅ‚nym logowaniem i bezpiecznym LLM
    """
    start_time = time.time()
    intencja = None
    model_llm = None
    odpowiedz = ""
    
    print("ğŸ“¥ Otrzymano polecenie:", tekst)

    try:
        # === KROK 1: PrÃ³ba tradycyjnych wzorcÃ³w regex ===
        for komenda in KOMENDY:
            try:
                wzorzec = komenda["wzorzec"]
                intencja = komenda["intencja"]
                if re.search(wzorzec, tekst, re.IGNORECASE):
                    print(f"âœ… Rozpoznano intencjÄ™ (regex): {intencja}")
                    
                    # Loguj rozpoznanÄ… intencjÄ™
                    logger.loguj_intencje(intencja, tekst)
                    
                    # Wykonaj intencjÄ™
                    odpowiedz = wykonaj_intencje(intencja, tekst, tts_module)
                    
                    # SprawdÅº czy to byÅ‚a implementacja lokalna czy LLM fallback
                    if "ğŸ¤– [LLM za intencjÄ™" in odpowiedz:
                        # To byÅ‚ LLM fallback - juÅ¼ zalogowane w wykonaj_intencje
                        return odpowiedz
                    else:
                        # Lokalna implementacja
                        logger.loguj_rozmowe(
                            tekst_wej=tekst,
                            tekst_wyj=odpowiedz,
                            intencja=intencja,
                            model_llm=None,
                            czas_start=start_time,
                            metadata={"typ": "intencja_lokalna_regex", "wzorzec": wzorzec}
                        )
                    
                    return odpowiedz
                    
            except KeyError as e:
                logger.loguj_blad("config_error", f"BÅ‚Ä™dna struktura komendy: brak klucza {e}", komenda)
                continue
            except re.error as e:
                logger.loguj_blad("regex_error", f"BÅ‚Ä™dny wzorzec regex: {e}", wzorzec)
                continue

        # === KROK 2: JeÅ›li regex nie zadziaÅ‚aÅ‚, sprÃ³buj LLM Intent Classifier ===
        print("ğŸ” Regex nie rozpoznaÅ‚ - prÃ³bujÄ™ LLM Intent Classifier...")
        
        llm_intencja = klasyfikuj_intencje_llm(tekst, KOMENDY)
        
        if llm_intencja:
            print(f"âœ… Rozpoznano intencjÄ™ (LLM): {llm_intencja}")
            
            # Loguj rozpoznanÄ… intencjÄ™
            logger.loguj_intencje(llm_intencja, tekst)
            
            # Wykonaj intencjÄ™
            odpowiedz = wykonaj_intencje(llm_intencja, tekst, tts_module)
            
            # SprawdÅº czy to byÅ‚a implementacja lokalna czy LLM fallback
            if "ğŸ¤– [LLM za intencjÄ™" in odpowiedz:
                # To byÅ‚ LLM fallback - juÅ¼ zalogowane
                return odpowiedz
            else:
                # Lokalna implementacja przez LLM classifier
                logger.loguj_rozmowe(
                    tekst_wej=tekst,
                    tekst_wyj=odpowiedz,
                    intencja=llm_intencja,
                    model_llm="gpt-3.5-turbo",  # Model uÅ¼yty do klasyfikacji
                    czas_start=start_time,
                    metadata={"typ": "intencja_lokalna_llm_classifier"}
                )
            
            return odpowiedz

        # Brak dopasowania â†’ LLM z bezpiecznym systemem tokenÃ³w
        print("ğŸ¤– Brak predefiniowanej komendy â€“ pytam LLM...")
        
        try:
            model_llm = config["llm_config"]["model"]
            
            # NOWE: UÅ¼yj bezpiecznej funkcji LLM z fallback
            odpowiedz = zapytaj_llm_safe_with_fallback(tekst, config)
            
            print(f"ğŸ§  LLM odpowiada: {odpowiedz}")
            
            # SprawdÅº czy odpowiedÅº nie jest bÅ‚Ä™dem
            if odpowiedz.startswith("âŒ") or odpowiedz.startswith("[BÅ‚Ä…d"):
                logger.loguj_blad("llm_error", odpowiedz, {"tekst": tekst, "model": model_llm})
                if "Brak kredytÃ³w API" in odpowiedz:
                    # Zachowaj oryginalnÄ… wiadomoÅ›Ä‡ o brakach kredytÃ³w
                    pass  # nie zmieniaj odpowiedzi
                else:
                    odpowiedz = "Przepraszam, wystÄ…piÅ‚ problem z poÅ‚Ä…czeniem. SprÃ³buj ponownie."
            
            # Wypowiedz odpowiedÅº
            tts_module.mow_tekstem(odpowiedz)
            
            # Loguj rozmowÄ™ LLM
            logger.loguj_rozmowe(
                tekst_wej=tekst,
                tekst_wyj=odpowiedz,
                intencja=None,
                model_llm=model_llm,
                czas_start=start_time,
                metadata={"typ": "llm_response"}
            )
            
        except Exception as e:
            error_msg = f"Przepraszam, wystÄ…piÅ‚ bÅ‚Ä…d podczas przetwarzania: {str(e)}"
            logger.loguj_blad("llm_processing_error", str(e), {"tekst": tekst})
            
            tts_module.mow_tekstem(error_msg)
            
            # Loguj bÅ‚Ä…d jako rozmowÄ™
            logger.loguj_rozmowe(
                tekst_wej=tekst,
                tekst_wyj=error_msg,
                intencja=None,
                model_llm=model_llm,
                czas_start=start_time,
                metadata={"typ": "error_response", "error": str(e)}
            )
            
            odpowiedz = error_msg
        
    except Exception as e:
        error_msg = f"WystÄ…piÅ‚ nieoczekiwany bÅ‚Ä…d systemu."
        logger.loguj_blad("system_error", str(e), {"tekst": tekst, "function": "analizuj"})
        
        tts_module.mow_tekstem(error_msg)
        odpowiedz = error_msg
    
    return odpowiedz

# === 4. Wykonanie intencji (bez zmian) ===
def wykonaj_intencje(intencja, tekst, tts_module):
    """
    Wykonuje konkretnÄ… intencjÄ™ z logowaniem
    """
    odpowiedz = ""
    
    try:
        # === ISTNIEJÄ„CE INTENCJE ===
        if intencja == "zapytanie_godzina":
            from datetime import datetime
            godzina = datetime.now().strftime("%H:%M")
            odpowiedz = f"Jest godzina {godzina}"
            
        elif intencja == "zapytanie_data":
            from datetime import datetime
            data = datetime.now().strftime("%d %B %Y")
            odpowiedz = f"Dzisiaj jest {data}"
            
        elif intencja == "powitanie":
            import random
            powitania = [
                "CzeÅ›Ä‡! Jak mogÄ™ pomÃ³c?",
                "Witaj! W czym mogÄ™ Ci pomÃ³c?",
                "Hej! Gotowy do pracy!"
            ]
            odpowiedz = random.choice(powitania)
            
        elif intencja == "pozegnanie":
            import random
            pozegnania = [
                "Do widzenia!",
                "MiÅ‚ego dnia!",
                "Do zobaczenia!"
            ]
            odpowiedz = random.choice(pozegnania)
            
        elif intencja == "status_systemu":
            import torch
            gpu_status = "aktywne" if torch.cuda.is_available() else "nieaktywne"
            rag_status = "dostÄ™pny" if RAG_AVAILABLE else "niedostÄ™pny"
            
            # NOWE: Dodaj info o limitach tokenÃ³w
            cached_models = len(token_manager.model_limits)
            odpowiedz = f"System AIA dziaÅ‚a poprawnie. GPU: {gpu_status}, RAG: {rag_status}, Limity tokenÃ³w: {cached_models} modeli w cache"
            
        elif intencja == "zapisz_wiadomosc":
            # WyciÄ…gnij treÅ›Ä‡ wiadomoÅ›ci z tekstu
            import re
            match = re.search(r'(?:zapisz|dodaj|utw[oÃ³]rz).*wiadomo[Å›Ä‡s][Ä‡c]?\s*[":]\s*(.+)', tekst, re.IGNORECASE)
            if match:
                tresc = match.group(1).strip()
                from core.pamiec import zapisz_wiadomosc
                uuid_msg = zapisz_wiadomosc("Notatka gÅ‚osowa", tresc, "AIA UÅ¼ytkownik")
                odpowiedz = f"ZapisaÅ‚em wiadomoÅ›Ä‡: {tresc}"
            else:
                odpowiedz = "Nie zrozumiaÅ‚em co mam zapisaÄ‡. SprÃ³buj: 'Zapisz wiadomoÅ›Ä‡: treÅ›Ä‡'"
                
        elif intencja == "odczytaj_wiadomosc":
            from core.pamiec import pobierz_wiadomosci
            wiadomosci = pobierz_wiadomosci(limit=3)
            if wiadomosci:
                odpowiedz = "Twoje ostatnie wiadomoÅ›ci: "
                for i, w in enumerate(wiadomosci, 1):
                    odpowiedz += f"{i}. {w['tytul']}: {w['tresc']}. "
            else:
                odpowiedz = "Nie masz Å¼adnych zapisanych wiadomoÅ›ci."
                
        elif intencja == "przeglad_wiadomosci":
            from core.pamiec import pobierz_wiadomosci
            wiadomosci = pobierz_wiadomosci(limit=5)
            if wiadomosci:
                odpowiedz = f"Masz {len(wiadomosci)} wiadomoÅ›ci. "
                for i, w in enumerate(wiadomosci, 1):
                    odpowiedz += f"{i}. {w['tytul']} z {w['timestamp'][:10]}. "
            else:
                odpowiedz = "Nie masz Å¼adnych zapisanych wiadomoÅ›ci."
        
        # === NOWE INTENCJE KULINARNE ===
        elif intencja in ["co_moge_zrobic_z_lodowki", "zaproponuj_dania", "dania_z_skladnikow"]:
            odpowiedz = obsÅ‚uga_rag_ogolna(tekst, intencja)
            
        elif intencja == "dania_wege":
            odpowiedz = obsÅ‚uga_rag_kategoria(tekst, "wege")
            
        elif intencja == "dania_niskotluszczowe":
            odpowiedz = obsÅ‚uga_rag_kategoria(tekst, "niskotÅ‚uszczowa")
            
        elif intencja == "dania_niskocukrowe":
            odpowiedz = obsÅ‚uga_rag_kategoria(tekst, "niskocukrowa")
            
        elif intencja == "kalorie_przepisu":
            odpowiedz = "Funkcja liczenia kalorii bÄ™dzie wkrÃ³tce dostÄ™pna."
            
        elif intencja == "kalorie_produktu":
            odpowiedz = oblicz_kalorie_produktu(tekst)
            
        elif intencja == "przepis_szczegolowy":
            odpowiedz = "SzczegÃ³Å‚owe przepisy bÄ™dÄ… dostÄ™pne w przyszÅ‚ej wersji."
            
        elif intencja == "skladniki_na_danie":
            odpowiedz = "Funkcja wyÅ›wietlania skÅ‚adnikÃ³w bÄ™dzie wkrÃ³tce dostÄ™pna."
            
        else:
            # NOWE: Nieznana intencja â†’ przekaÅ¼ do LLM
            print(f"â“ Nieznana intencja '{intencja}' - przekazujÄ™ do LLM...")
            
            try:
                model_llm = "LLM_fallback"  # Oznacz Å¼e to fallback
                odpowiedz_llm = zapytaj_llm_safe_with_fallback(tekst, {"llm_config": {"model": "openai/gpt-4-turbo", "max_tokens": 2048, "temperature": 0.65}})
                
                # Dodaj prefix Å¼eby byÅ‚o widaÄ‡ ÅºrÃ³dÅ‚o
                odpowiedz = f"ğŸ¤– [LLM za intencjÄ™ '{intencja}']: {odpowiedz_llm}"
                
                # Zaloguj jako hybrydowÄ… odpowiedÅº
                logger.loguj_rozmowe(
                    tekst_wej=tekst,
                    tekst_wyj=odpowiedz,
                    intencja=intencja,
                    model_llm=model_llm,
                    czas_start=time.time(),
                    metadata={"typ": "intencja_llm_fallback", "original_intent": intencja}
                )
                
            except Exception as e:
                odpowiedz = f"âŒ ZrozumiaÅ‚em intencjÄ™ '{intencja}', ale wystÄ…piÅ‚ bÅ‚Ä…d podczas przetwarzania: {str(e)}"
                logger.loguj_blad("intent_llm_fallback_error", str(e), {"intencja": intencja, "tekst": tekst})
            
    except Exception as e:
        logger.loguj_blad("intencja_error", f"BÅ‚Ä…d podczas wykonywania intencji {intencja}: {e}", {"tekst": tekst})
        odpowiedz = "Przepraszam, wystÄ…piÅ‚ bÅ‚Ä…d podczas wykonywania polecenia."

    # WyÅ›wietl i wypowiedz odpowiedÅº
    print(f"ğŸ—£ï¸ AIA odpowiada: {odpowiedz}")
    try:
        tts_module.mow_tekstem(odpowiedz)
    except Exception as e:
        logger.loguj_blad("tts_error", f"BÅ‚Ä…d TTS: {e}", {"odpowiedz": odpowiedz})

    return odpowiedz

# === 5. Funkcja kalorii ===
def oblicz_kalorie_produktu(tekst):
    """WyciÄ…ga produkt i zwraca kalorie z bazy skÅ‚adnikÃ³w"""
    import re
    match = re.search(r'ile.*kalorii.*ma.*?([a-zA-ZÄ…Ä‡Ä™Å‚Å„Ã³Å›ÅºÅ¼Ä„Ä†Ä˜ÅÅƒÃ“ÅšÅ¹Å»]+)', tekst, re.IGNORECASE)
    
    if not match:
        return "Nie zrozumiaÅ‚em o jaki produkt pytasz."
    
    produkt = match.group(1).lower()
    print(f"ğŸ” Szukam kalorii dla: {produkt}")
    
    # SprawdÅº w bazie skÅ‚adnikÃ³w
    if RAG_AVAILABLE and recipe_rag:
        try:
            skladniki = recipe_rag.engine.loader.skladniki
            if produkt in skladniki:
                kalorie = skladniki[produkt]['kalorie_na_100g']
                waga = skladniki[produkt]['waga_standardowa'] 
                jednostka = skladniki[produkt]['jednostka']
                return f"{produkt.capitalize()} ma {kalorie} kalorii na 100g. Standardowa porcja ({waga}g/{jednostka}) to {int(kalorie * waga / 100)} kalorii."
            else:
                return f"Nie mam informacji o kaloriach dla {produkt}. DostÄ™pne produkty: jajka, papryka, pomidor, Å‚osoÅ›, tofu..."
        except Exception as e:
            print(f"âŒ BÅ‚Ä…d sprawdzania kalorii: {e}")
            return "WystÄ…piÅ‚ bÅ‚Ä…d podczas sprawdzania kalorii."
    else:
        return "System kalorii jest niedostÄ™pny."

# === 6. FUNKCJE RAG (bez zmian) ===
def wyciagnij_skladniki(tekst):
    """WyciÄ…ga skÅ‚adniki z tekstu uÅ¼ytkownika"""
    # Lista moÅ¼liwych skÅ‚adnikÃ³w (z bazy danych)
    mozliwe_skladniki = [
        "awokado", "banan", "brokuÅ‚y", "cebula", "chleb", "cukinia", "cytryna", 
        "czosnek", "imbir", "jajka", "jarmuÅ¼", "kapary", "kasza", "koperek",
        "kurczak", "marchewka", "mleko", "ocet", "ogÃ³rek", "oliwa", "papryka",
        "pieprz", "pomidor", "soczewica", "sos", "szpinak", "sÅ‚onecznik", "tofu", "Å‚osoÅ›"
    ]
    
    znalezione = []
    tekst_lower = tekst.lower()
    
    for skladnik in mozliwe_skladniki:
        if skladnik in tekst_lower:
            znalezione.append(skladnik)
    
    return znalezione

def obsÅ‚uga_rag_ogolna(tekst, intencja):
    """ObsÅ‚uga ogÃ³lnych zapytaÅ„ RAG"""
    if not RAG_AVAILABLE:
        return "Przepraszam, system przepisÃ³w jest chwilowo niedostÄ™pny."
    
    # WyciÄ…gnij skÅ‚adniki z tekstu
    skladniki = wyciagnij_skladniki(tekst)
    
    if not skladniki:
        return "Nie rozpoznaÅ‚em Å¼adnych skÅ‚adnikÃ³w. SprÃ³buj: 'mam jajka, paprykÄ™, pomidory'"
    
    print(f"ğŸ” Znalezione skÅ‚adniki: {skladniki}")
    
    # Wyszukaj przepisy
    result = recipe_rag.suggest_recipes(skladniki, max_results=3)
    
    if result.get('found_recipes', 0) == 0:
        return f"Nie znalazÅ‚em przepisÃ³w z {', '.join(skladniki)}. SprÃ³buj innych skÅ‚adnikÃ³w."
    
    # SformuÅ‚uj odpowiedÅº
    odpowiedz = f"Z {', '.join(skladniki)} mogÄ™ zaproponowaÄ‡: "
    
    for category, recipes in result.get('by_category', {}).items():
        cat_name = {"wege": "wegetariaÅ„skie", "niskotÅ‚uszczowa": "niskotÅ‚uszczowe", 
                   "niskocukrowa": "niskocukrowe", "keto": "keto"}.get(category, category)
        
        recipe_names = [r['title'] for r in recipes[:2]]  # max 2 z kategorii
        odpowiedz += f"{cat_name}: {', '.join(recipe_names)}. "
    
    return odpowiedz

def obsÅ‚uga_rag_kategoria(tekst, kategoria):
    """ObsÅ‚uga zapytaÅ„ z konkretnÄ… kategoriÄ…"""
    if not RAG_AVAILABLE:
        return "Przepraszam, system przepisÃ³w jest chwilowo niedostÄ™pny."
    
    # WyciÄ…gnij skÅ‚adniki jeÅ›li sÄ…
    skladniki = wyciagnij_skladniki(tekst)
    
    if not skladniki:
        # Brak skÅ‚adnikÃ³w - pokaÅ¼ ogÃ³lne przepisy z kategorii
        result = recipe_rag.suggest_recipes([''], category=kategoria, max_results=3)
    else:
        # Z konkretnymi skÅ‚adnikami
        result = recipe_rag.suggest_recipes(skladniki, category=kategoria, max_results=3)
    
    if result.get('found_recipes', 0) == 0:
        cat_name = {"wege": "wegetariaÅ„skich", "niskotÅ‚uszczowa": "niskotÅ‚uszczowych", 
                   "niskocukrowa": "niskocukrowych"}.get(kategoria, kategoria)
        return f"Nie znalazÅ‚em {cat_name} przepisÃ³w. SprÃ³buj innych skÅ‚adnikÃ³w."
    
    # SformuÅ‚uj odpowiedÅº
    cat_name = {"wege": "wegetariaÅ„skie", "niskotÅ‚uszczowa": "niskotÅ‚uszczowe", 
               "niskocukrowa": "niskocukrowe"}.get(kategoria, kategoria)
    
    recipes = result.get('all_recipes', [])
    recipe_names = [r['title'] for r in recipes[:3]]
    
    if skladniki:
        odpowiedz = f"Dania {cat_name} z {', '.join(skladniki)}: {', '.join(recipe_names)}"
    else:
        odpowiedz = f"ProponujÄ™ {cat_name} dania: {', '.join(recipe_names)}"
    
    return odpowiedz

# === 7. Funkcje pomocnicze ===
def dodaj_komende(wzorzec, intencja):
    """Dodaje nowÄ… komendÄ™ do listy (w runtime)"""
    KOMENDY.append({"wzorzec": wzorzec, "intencja": intencja})
    print(f"âœ… Dodano komendÄ™: {wzorzec} -> {intencja}")

def lista_intencji():
    """Zwraca listÄ™ wszystkich dostÄ™pnych intencji"""
    return [komenda["intencja"] for komenda in KOMENDY]

# NOWA FUNKCJA: Info o tokenach
def sprawdz_limity_tokenow():
    """Zwraca informacje o zapisanych limitach tokenÃ³w"""
    return token_manager.model_limits

def wyczysc_cache_tokenow():
    """CzyÅ›ci cache limitÃ³w tokenÃ³w"""
    token_manager.model_limits = {}
    token_manager.save_cache()
    print("ğŸ§¹ Wyczyszczono cache limitÃ³w tokenÃ³w")

# === Test lokalny ===
if __name__ == "__main__":
    print("ğŸ§ª Test moduÅ‚u rozumienie z RAG i bezpiecznymi tokenami")
    
    # Mock config i TTS dla testÃ³w
    mock_config = {
        "local_config": {"styl": "precyzyjny"},
        "llm_config": {"model": "openai/gpt-4-turbo", "max_tokens": 2048}
    }
    
    class MockTTS:
        def mow_tekstem(self, tekst):
            print(f"[MOCK TTS]: {tekst}")
    
    mock_tts = MockTTS()
    
    # Testy
    print("\n--- Test 1: Zapytanie o godzinÄ™ ---")
    analizuj("ktÃ³ra godzina?", mock_config, mock_tts)
    
    print("\n--- Test 2: RAG - skÅ‚adniki ---")
    analizuj("mam jajka i paprykÄ™ co mogÄ™ zrobiÄ‡", mock_config, mock_tts)
    
    print("\n--- Test 3: Status systemu z tokenami ---") 
    analizuj("status systemu", mock_config, mock_tts)
    
    print("\n--- Test 4: SprawdÅº limity tokenÃ³w ---")
    print("Zapisane limity:", sprawdz_limity_tokenow())