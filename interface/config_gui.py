# ===================================================================
# INTERFACE/CONFIG_GUI.PY - KONFIGURACJA AIA V2 PRZEZ STREAMLIT
# ===================================================================
# Wersja: AIA v2.1 - UPROSZCZONA Z OLLAMA SUPPORT
# Opis: GUI do konfiguracji wszystkich komponent√≥w systemu AIA
# Komponenty: STT, TTS, LLM Provider Switcher, Recognition Methods, Parametry
# ===================================================================

# === 1. IMPORTY I INICJALIZACJA SESJI ===
import streamlit as st
import json
import os
import subprocess

# Inicjalizacja stanu sesji - ≈õledzenie czy AIA zosta≈Ça uruchomiona
if "aia_uruchomiono" not in st.session_state:
    st.session_state["aia_uruchomiono"] = False

# === 2. ≈öCIE≈ªKI DO PLIK√ìW KONFIGURACYJNYCH ===
config_path = "config/config.json"        # G≈Ç√≥wna konfiguracja systemu

# === 3. WCZYTYWANIE ISTNIEJƒÑCEJ KONFIGURACJI ===
config = {}  # G≈Ç√≥wna konfiguracja

# Wczytaj config.json je≈õli istnieje
if os.path.exists(config_path):
    with open(config_path, "r", encoding="utf-8") as f:
        config = json.load(f)

# Konfiguracja strony Streamlit
st.set_page_config(page_title="AIA v2 ‚Äì Konfiguracja", layout="centered")
st.title("üß† AIA v2 ‚Äì Konfiguracja systemu")

# ===================================================================
# ü§ñ SEKCJA 4: LLM PROVIDER SELECTOR - NOWA SEKCJA
# ===================================================================

st.header("ü§ñ Wyb√≥r LLM Provider")

# Pobierz aktualny provider z config
current_provider = config.get("llm_config", {}).get("provider", "openrouter")
provider_index = 0 if current_provider == "openrouter" else 1

# Wyb√≥r provider
llm_provider = st.radio(
    "Wybierz provider:",
    options=["OpenRouter (chmura)", "Ollama (lokalny)"],
    index=provider_index,
    help="OpenRouter = modele w chmurze (p≈Çatne)\nOllama = modele lokalne (darmowe)"
)

# Mapowanie do warto≈õci config
if llm_provider == "Ollama (lokalny)":
    provider = "ollama"
    st.success("üÜì Darmowy lokalny LLM")
else:
    provider = "openrouter" 
    st.info("üí∞ P≈Çatne modele w chmurze")

# ===================================================================
# DYNAMICZNY WYB√ìR MODELU NA PODSTAWIE PROVIDER
# ===================================================================

if provider == "ollama":
    # SEKCJA OLLAMA
    st.subheader("ü¶ô Ollama Models")
    
    # Sprawd≈∫ dostƒôpne modele Ollama
    try:
        import requests
        response = requests.get("http://localhost:11434/api/tags", timeout=3)
        if response.status_code == 200:
            models_data = response.json()
            ollama_models = [model["name"] for model in models_data.get("models", [])]
            
            if ollama_models:
                # Znajd≈∫ aktualny model
                current_model = config.get("llm_config", {}).get("model", "llama3.1:8b")
                model_index = ollama_models.index(current_model) if current_model in ollama_models else 0
                
                llm_model = st.selectbox(
                    "Model lokalny:",
                    options=ollama_models,
                    index=model_index,
                    help="Modele pobrane lokalnie"
                )
                st.success(f"‚úÖ Ollama dzia≈Ça ({len(ollama_models)} modeli)")
            else:
                st.error("‚ùå Brak modeli Ollama")
                st.code("ollama pull llama3.1:8b")
                llm_model = "llama3.1:8b"  # Fallback
                
        else:
            st.error("‚ùå Ollama server nie odpowiada")
            st.code("ollama serve")
            llm_model = "llama3.1:8b"  # Fallback
            
    except Exception as e:
        st.error("‚ùå Ollama niedostƒôpny")
        st.code("ollama serve")
        llm_model = "llama3.1:8b"  # Fallback
    
    # Ustawienia Ollama
    current_base_url = config.get("llm_config", {}).get("base_url", "http://localhost:11434")
    base_url = st.text_input(
        "Ollama URL:",
        value=current_base_url,
        help="Adres serwera Ollama"
    )
    
else:
    # SEKCJA OPENROUTER (ISTNIEJƒÑCY KOD)
    st.subheader("üåê OpenRouter Models")
    
    openrouter_models = [
        "openai/gpt-3.5-turbo",
        "openai/gpt-4-turbo", 
        "openai/gpt-4",
        "mistralai/mistral-7b-instruct",
        "anthropic/claude-3-sonnet"
    ]
    
    # Znajd≈∫ aktualny model
    current_model = config.get("llm_config", {}).get("model", "openai/gpt-3.5-turbo")
    model_index = openrouter_models.index(current_model) if current_model in openrouter_models else 0
    
    llm_model = st.selectbox(
        "Model LLM:",
        options=openrouter_models,
        index=model_index,
        help="Wybierz model jƒôzykowy"
    )
    
    base_url = None  # OpenRouter nie potrzebuje base_url

# Status panel - info o aktualnym provider
st.markdown("---")
col1, col2 = st.columns(2)
with col1:
    st.metric("Provider", provider.upper())
    st.metric("Model", llm_model.split("/")[-1] if "/" in llm_model else llm_model.split(":")[0])
with col2:
    st.metric("Koszt", "Darmowy" if provider == "ollama" else "P≈Çatny")
    st.metric("Lokalizacja", "GPU" if provider == "ollama" else "Chmura")

# ===================================================================
# üß† SEKCJA 5: METODY ROZPOZNAWANIA INTENCJI
# ===================================================================

st.header("üéõÔ∏è Metody rozpoznawania intencji")

# Pobierz aktualnƒÖ konfiguracjƒô rozpoznawania
recognition_config = config.get("recognition_config", {})

# Wyb√≥r metody rozpoznawania
recognition_methods = {
    "regex_only": "üèÉ‚Äç‚ôÇÔ∏è Tylko Regex (najszybsze)",
    "regex_plus_simple": "ü§ñ Regex + Prosty LLM (standardowe)", 
    "regex_plus_few_shot": "üß† Regex + Few-shot LLM (najlepsze)",
}

current_method = recognition_config.get("method", "regex_plus_simple")
method_index = list(recognition_methods.keys()).index(current_method) if current_method in recognition_methods.keys() else 1

recognition_method = st.selectbox(
    "Metoda rozpoznawania:",
    options=list(recognition_methods.keys()),
    format_func=lambda x: recognition_methods[x],
    index=method_index,
    help="""
    ‚Ä¢ Regex Only: Tylko wzorce - bardzo szybkie, ograniczone
    ‚Ä¢ Regex + Simple: Wzorce + prosty LLM - dobry kompromis  
    ‚Ä¢ Regex + Few-shot: Wzorce + inteligentny LLM - najlepsze rozumienie
    """
)

# Zaawansowane ustawienia rozpoznawania
with st.expander("‚öôÔ∏è Zaawansowane ustawienia rozpoznawania"):
    # Pr√≥g pewno≈õci klasyfikacji
    confidence_threshold = st.slider(
        "üéØ Pr√≥g pewno≈õci klasyfikacji",
        min_value=0.0,
        max_value=1.0,
        value=float(recognition_config.get("confidence_threshold", 0.7)),
        step=0.05,
        help="Poni≈ºej tego progu system przejdzie do czystego LLM"
    )
    
    # Kontekst z poprzedniej rozmowy
    use_context = st.checkbox(
        "üí≠ U≈ºywaj kontekstu poprzedniej rozmowy",
        value=recognition_config.get("use_context", False),
        help="System bƒôdzie pamiƒôtaƒá poprzedniƒÖ wymianƒô zda≈Ñ"
    )
    
    # Tryb debug
    debug_mode = st.checkbox(
        "üîç Tryb debug (poka≈º kroki klasyfikacji)",
        value=recognition_config.get("debug_mode", False),
        help="Wy≈õwietla szczeg√≥≈Çowe informacje o procesie rozpoznawania"
    )

# Informacyjny box o wybranej metodzie
if recognition_method == "regex_only":
    st.info("‚ö° **Regex Only**: Najszybsza metoda. Rozpoznaje tylko dok≈Çadne wzorce z pliku komendy_domyslne.json")
elif recognition_method == "regex_plus_simple":
    st.info("üîÑ **Regex + Simple LLM**: Standardowa metoda. Regex + prosty prompt do LLM")
elif recognition_method == "regex_plus_few_shot":
    st.success("üéØ **Regex + Few-shot LLM**: Najlepsza metoda. Regex + inteligentny LLM z przyk≈Çadami")

# ===================================================================
# üîß SEKCJA 6: KOMPONENTY SYSTEMU - STT, TTS
# ===================================================================

st.header("üîß Komponenty systemu")

# === STT (Speech-to-Text) Configuration ===
st.subheader("üó£Ô∏è Rozpoznawanie mowy (STT)")

stt_options = ["whisper", "faster_whisper", "vosk", "google"]
current_stt = config.get("local_config", {}).get("stt", "faster_whisper")
stt_index = stt_options.index(current_stt) if current_stt in stt_options else 1

stt = st.selectbox(
    "Silnik STT:",
    stt_options,
    index=stt_index,
    help="""
    ‚Ä¢ whisper: OpenAI Whisper (dok≈Çadny, wymaga internetu)
    ‚Ä¢ faster_whisper: Zoptymalizowany Whisper (zalecany, dzia≈Ça offline)  
    ‚Ä¢ vosk: Lokalny model (szybki, mniej dok≈Çadny)
    ‚Ä¢ google: Google Speech API (wymaga klucza API)
    """
)

# === TTS (Text-to-Speech) Configuration ===
st.subheader("üîä Synteza mowy (TTS)")

tts_options = ["edge"]
current_tts = config.get("local_config", {}).get("tts", "edge")
tts_index = tts_options.index(current_tts) if current_tts in tts_options else 0

tts = st.selectbox(
    "Silnik TTS:", 
    tts_options, 
    index=tts_index,
    help="Obecnie dostƒôpny tylko Edge-TTS z powodu najlepszej jako≈õci polskiego g≈Çosu"
)

# Konfiguracja Edge-TTS - polskie g≈Çosy
if tts == "edge":
    edge_voices = ["zofia", "marek"]
    voice_descriptions = {
        "zofia": "üë© Zofia - g≈Ços kobiecy (naturalny, przyjemny)", 
        "marek": "üë® Marek - g≈Ços mƒôski (spokojny, profesjonalny)"
    }
    
    current_edge_voice = config.get("local_config", {}).get("edge_voice", "marek")
    edge_voice_index = edge_voices.index(current_edge_voice) if current_edge_voice in edge_voices else 1
    
    edge_voice = st.selectbox(
        "G≈Ços polski:",
        edge_voices,
        index=edge_voice_index,
        format_func=lambda x: voice_descriptions[x],
        help="Wybierz preferowany g≈Ços dla systemu AIA"
    )
else:
    edge_voice = "marek"  # Warto≈õƒá domy≈õlna

# ===================================================================
# üéõÔ∏è SEKCJA 7: PARAMETRY GENEROWANIA LLM
# ===================================================================

st.header("üéõÔ∏è Parametry generowania")

llm_config = config.get("llm_config", {})

# Max tokens - maksymalna d≈Çugo≈õƒá odpowiedzi
max_tokens = st.slider(
    "üéØ Maksymalna d≈Çugo≈õƒá odpowiedzi (tokens)", 
    min_value=128, 
    max_value=8192, 
    value=llm_config.get("max_tokens", 2048), 
    step=64,
    help="Wiƒôksza warto≈õƒá = d≈Çu≈ºsze odpowiedzi, ale wiƒôkszy koszt/czas"
)

# Temperature - kreatywno≈õƒá odpowiedzi
temperature = st.slider(
    "üî• Temperature (kreatywno≈õƒá)", 
    min_value=0.0, 
    max_value=1.5, 
    value=float(llm_config.get("temperature", 0.7)), 
    step=0.05,
    help="0.0 = deterministyczne, 1.0 = bardzo kreatywne"
)

# Top-p - nucleus sampling (tylko dla OpenRouter)
if provider == "openrouter":
    top_p = st.slider(
        "üé≤ Top-p (nucleus sampling)", 
        min_value=0.0, 
        max_value=1.0, 
        value=float(llm_config.get("top_p", 1.0)), 
        step=0.05,
        help="1.0 = wszystkie tokeny, 0.9 = top 90% prawdopodobnych"
    )
else:
    top_p = 1.0  # Warto≈õƒá domy≈õlna dla Ollama

# ===================================================================
# ‚öôÔ∏è SEKCJA 8: TRYB I STYL DZIA≈ÅANIA
# ===================================================================

st.header("‚öôÔ∏è Tryb dzia≈Çania")

# Tryby pracy systemu
tryby = ["standardowy", "testowy", "prezentacja", "domowy"]
current_tryb = config.get("local_config", {}).get("tryb", "testowy")
tryb_index = tryby.index(current_tryb) if current_tryb in tryby else 1

tryb = st.selectbox(
    "Tryb pracy:", 
    tryby,
    index=tryb_index,
    help="""
    ‚Ä¢ standardowy: Normalna praca systemu
    ‚Ä¢ testowy: Rozszerzone logowanie i debugowanie  
    ‚Ä¢ prezentacja: Tryb demonstracyjny
    ‚Ä¢ domowy: Optymalizacja dla u≈ºytku domowego
    """
)

# Styl odpowiedzi AI
styl_options = ["precyzyjny", "kreatywny"]
current_styl = config.get("local_config", {}).get("styl", "precyzyjny")
styl_index = styl_options.index(current_styl) if current_styl in styl_options else 0

styl = st.radio(
    "üé® Styl odpowiedzi", 
    styl_options,
    index=styl_index,
    help="""
    ‚Ä¢ precyzyjny: Konkretne, rzeczowe odpowiedzi
    ‚Ä¢ kreatywny: Bardziej rozbudowane, emocjonalne odpowiedzi
    """
)

# ===================================================================
# üíæ SEKCJA 10: ZAPISZ I URUCHOM
# ===================================================================

st.markdown("---")
st.header("üíæ Zapisz i uruchom")

# PodglƒÖd konfiguracji
with st.expander("üëÅÔ∏è PodglƒÖd konfiguracji"):
    if provider == "ollama":
        preview_llm_config = {
            "provider": "ollama",
            "model": llm_model,
            "base_url": base_url,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "alternative_models": []
        }
    else:
        preview_llm_config = {
            "provider": "openrouter",
            "model": llm_model,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "top_p": top_p,
            "alternative_models": []
        }
    
    preview_config = {
        "llm_config": preview_llm_config,
        "local_config": {
            "tryb": tryb,
            "styl": styl,
            "stt": stt,
            "tts": tts,
            "edge_voice": edge_voice
        },
        "recognition_config": {
            "method": recognition_method,
            "confidence_threshold": confidence_threshold,
            "use_context": use_context,
            "debug_mode": debug_mode
        }
    }
    st.json(preview_config)

# Kolumny dla przycisk√≥w
col1, col2 = st.columns(2)

with col1:
    # Przycisk zapisu konfiguracji
    if st.button("üíæ Zapisz konfiguracjƒô", type="primary"):
        if provider == "ollama":
            new_llm_config = {
                "provider": "ollama",
                "model": llm_model,
                "base_url": base_url,
                "max_tokens": max_tokens,
                "temperature": temperature,
                "alternative_models": []
            }
        else:
            new_llm_config = {
                "provider": "openrouter",
                "model": llm_model,
                "max_tokens": max_tokens,
                "temperature": temperature,
                "top_p": top_p,
                "alternative_models": []
            }
        
        new_config = {
            "llm_config": new_llm_config,
            "local_config": {
                "tryb": tryb,
                "styl": styl,
                "stt": stt,
                "tts": tts,
                "edge_voice": edge_voice
            },
            "recognition_config": {
                "method": recognition_method,
                "confidence_threshold": confidence_threshold,
                "use_context": use_context,
                "debug_mode": debug_mode
            }
        }
        
        try:
            # Zapisz konfiguracjƒô do pliku
            with open(config_path, "w", encoding="utf-8") as f:
                json.dump(new_config, f, indent=4, ensure_ascii=False)
            st.success("‚úÖ Konfiguracja zosta≈Ça zapisana pomy≈õlnie!")
            st.info(f"üìÅ Zapisano w: {config_path}")
            
        except Exception as e:
            st.error(f"‚ùå B≈ÇƒÖd podczas zapisu konfiguracji: {e}")

with col2:
    # Przycisk uruchomienia
    if st.button("üöÄ Zapisz i uruchom AIA", type="secondary"):
        # Najpierw zapisz konfiguracjƒô (kod identyczny jak wy≈ºej)
        if provider == "ollama":
            new_llm_config = {
                "provider": "ollama",
                "model": llm_model,
                "base_url": base_url,
                "max_tokens": max_tokens,
                "temperature": temperature,
                "alternative_models": []
            }
        else:
            new_llm_config = {
                "provider": "openrouter",
                "model": llm_model,
                "max_tokens": max_tokens,
                "temperature": temperature,
                "top_p": top_p,
                "alternative_models": []
            }
        
        new_config = {
            "llm_config": new_llm_config,
            "local_config": {
                "tryb": tryb,
                "styl": styl,
                "stt": stt,
                "tts": tts,
                "edge_voice": edge_voice
            },
            "recognition_config": {
                "method": recognition_method,
                "confidence_threshold": confidence_threshold,
                "use_context": use_context,
                "debug_mode": debug_mode
            }
        }
        
        try:
            # Zapisz konfiguracjƒô
            with open(config_path, "w", encoding="utf-8") as f:
                json.dump(new_config, f, indent=4, ensure_ascii=False)
            
            st.success("‚úÖ Konfiguracja zapisana!")
            
            # Uruchom AIA
            subprocess.Popen(["python", "main.py"], shell=True)
            st.success("‚úÖ System AIA zosta≈Ç uruchomiony!")
            st.info("üß† AIA dzia≈Ça zgodnie z konfiguracjƒÖ. Sprawd≈∫ terminal.")
            
            # Animacja sukcesu
            st.balloons()
            
        except Exception as e:
            st.error(f"‚ùå B≈ÇƒÖd: {e}")

# Instrukcje dla u≈ºytkownika
st.markdown("### üì¢ Co dalej?")
st.info("""
üé§ **Powiedz:** "Stefan" aby aktywowaƒá system

üó£Ô∏è **Przyk≈Çadowe komendy:**
‚Ä¢ "Stefan, kt√≥rƒÖ mamy godzinƒô?"
‚Ä¢ "Stefan, ile kalorii ma pomidor?"
‚Ä¢ "Stefan, mam pomidor, co zrobiƒá?" (test RAG)
‚Ä¢ "Stefan, stop" - aby zako≈Ñczyƒá
""")

# ===================================================================
# üìù FOOTER
# ===================================================================

st.markdown("---")
st.caption("ü§ñ **AIA v2.1** - Asystent z obs≈ÇugƒÖ Ollama i OpenRouter")
st.caption(f"üí° Aktualny provider: **{provider.upper()}** | Model: **{llm_model}**")

# ===================================================================
# KONIEC PLIKU - UPROSZCZONA WERSJA CONFIG_GUI.PY
# ===================================================================